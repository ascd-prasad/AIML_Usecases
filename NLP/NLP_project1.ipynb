{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HfuEd1qlKAjD"
   },
   "outputs": [],
   "source": [
    "#Import all the necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(style= 'darkgrid')\n",
    "import random , re , string\n",
    "import nltk\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTYRkP9Ks8mS",
    "outputId": "7cec7560-6c46-4a39-e3d9-fb3cb4d003a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec7aJ0PHKZaJ",
    "outputId": "b1d706e0-e7ad-4b6d-b3de-edb05be8d292"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhCRSgDmKke3"
   },
   "source": [
    "# ***PART 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEe7TcpjKpsF",
    "outputId": "6237d2e2-4484-41c2-d1c7-2fe100d22b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train set (681284, 7)\n"
     ]
    }
   ],
   "source": [
    "trainData = []\n",
    "\n",
    "# Load data to file\n",
    "trainData = pd.read_csv(\"/content/drive/MyDrive/AIML/LABS/NLP/Project 1/blogtext.csv\")\n",
    "\n",
    "print(\"Shape of Train set\",trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dyj_niB1fUDL",
    "outputId": "be989612-dd75-4e16-e484-1823e4c13ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 681284 entries, 0 to 681283\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   id      681284 non-null  int64 \n",
      " 1   gender  681284 non-null  object\n",
      " 2   age     681284 non-null  int64 \n",
      " 3   topic   681284 non-null  object\n",
      " 4   sign    681284 non-null  object\n",
      " 5   date    681284 non-null  object\n",
      " 6   text    681284 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 36.4+ MB\n"
     ]
    }
   ],
   "source": [
    "trainData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "iXvw3diaOkiq",
    "outputId": "20566322-9338-4c83-df64-c3e71fdd5df9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ...                                               text\n",
       "0  2059027  ...             Info has been found (+/- 100 pages,...\n",
       "1  2059027  ...             These are the team members:   Drewe...\n",
       "2  2059027  ...             In het kader van kernfusie op aarde...\n",
       "3  2059027  ...                   testing!!!  testing!!!          \n",
       "4  3581210  ...               Thanks to Yahoo!'s Toolbar I can ...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "2StlFFEaQ74Y",
    "outputId": "1a497a39-64ba-4c2a-8dde-768d3ade7117"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.812840e+05</td>\n",
       "      <td>681284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.397802e+06</td>\n",
       "      <td>23.932326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.247723e+06</td>\n",
       "      <td>7.786009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.114000e+03</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.239610e+06</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.607577e+06</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.525660e+06</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.337650e+06</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id            age\n",
       "count  6.812840e+05  681284.000000\n",
       "mean   2.397802e+06      23.932326\n",
       "std    1.247723e+06       7.786009\n",
       "min    5.114000e+03      13.000000\n",
       "25%    1.239610e+06      17.000000\n",
       "50%    2.607577e+06      24.000000\n",
       "75%    3.525660e+06      26.000000\n",
       "max    4.337650e+06      48.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "OyLhEod3UizV",
    "outputId": "9380a05f-70c5-4157-a4c5-a9ed304cbc81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "      <td>681284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>2616</td>\n",
       "      <td>611652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>male</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>Cancer</td>\n",
       "      <td>02,August,2004</td>\n",
       "      <td>urlLink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>345193</td>\n",
       "      <td>251015</td>\n",
       "      <td>65048</td>\n",
       "      <td>16544</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender   topic    sign            date                 text\n",
       "count   681284  681284  681284          681284               681284\n",
       "unique       2      40      12            2616               611652\n",
       "top       male  indUnk  Cancer  02,August,2004           urlLink   \n",
       "freq    345193  251015   65048           16544                  445"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mp-MnDmORs45",
    "outputId": "7e849276-a8d3-4fba-ec30-ffb280490ed0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing data\n",
    "trainData.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85i0bcM8SGYU",
    "outputId": "2858bedd-8735-48a4-e7ce-49c8e184b7bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 0.0%\n",
      "2, 26, 0.0%\n",
      "3, 40, 0.0%\n",
      "4, 12, 0.0%\n",
      "5, 2616, 0.4%\n"
     ]
    }
   ],
   "source": [
    "from numpy import unique\n",
    "list1=[]\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(trainData.shape[1]):\n",
    "    num = len(unique(trainData.iloc[:, i]))\n",
    "    percentage = float(num) / trainData.shape[0] * 100\n",
    "    if percentage < 1:\n",
    "        list1.append(i)\n",
    "        print('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BAn3RH5jTTWj",
    "outputId": "fa019df8-53c2-4709-cfeb-3bbcc2ae2d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique id values :\n",
      "[2059027 3581210 3539003 ... 3561647 4260903 1713845]\n",
      "\n",
      "unique gender values :\n",
      "['male' 'female']\n",
      "\n",
      "unique age values :\n",
      "[15 33 14 25 17 23 37 26 24 27 45 34 41 44 16 39 35 36 46 42 13 38 43 40\n",
      " 47 48]\n",
      "\n",
      "unique topic values :\n",
      "['Student' 'InvestmentBanking' 'indUnk' 'Non-Profit' 'Banking' 'Education'\n",
      " 'Engineering' 'Science' 'Communications-Media' 'BusinessServices'\n",
      " 'Sports-Recreation' 'Arts' 'Internet' 'Museums-Libraries' 'Accounting'\n",
      " 'Technology' 'Law' 'Consulting' 'Automotive' 'Religion' 'Fashion'\n",
      " 'Publishing' 'Marketing' 'LawEnforcement-Security' 'HumanResources'\n",
      " 'Telecommunications' 'Military' 'Government' 'Transportation'\n",
      " 'Architecture' 'Advertising' 'Agriculture' 'Biotech' 'RealEstate'\n",
      " 'Manufacturing' 'Construction' 'Chemicals' 'Maritime' 'Tourism'\n",
      " 'Environment']\n",
      "\n",
      "unique sign values :\n",
      "['Leo' 'Aquarius' 'Aries' 'Capricorn' 'Gemini' 'Cancer' 'Sagittarius'\n",
      " 'Scorpio' 'Libra' 'Virgo' 'Taurus' 'Pisces']\n",
      "\n",
      "unique date values :\n",
      "['14,May,2004' '13,May,2004' '12,May,2004' ... '05,august,2004'\n",
      " '04,august,2004' '02,august,2004']\n",
      "\n",
      "unique text values :\n",
      "['           Info has been found (+/- 100 pages, and 4.5 MB of .pdf files) Now i have to wait untill our team leader has processed it and learns html.         '\n",
      " '           These are the team members:   Drewes van der Laag           urlLink mail  Ruiyu Xie                     urlLink mail  Bryan Aaldering (me)          urlLink mail          '\n",
      " \"           In het kader van kernfusie op aarde:  MAAK JE EIGEN WATERSTOFBOM   How to build an H-Bomb From: ascott@tartarus.uwa.edu.au (Andrew Scott) Newsgroups: rec.humor Subject: How To Build An H-Bomb (humorous!) Date: 7 Feb 1994 07:41:14 GMT Organization: The University of Western Australia  Original file dated 12th November 1990. Seemed to be a transcript of a 'Seven Days' article. Poorly formatted and corrupted. I have added the text between 'examine under a microscope' and 'malleable, like gold,' as it was missing. If anyone has the full text, please distribute. I am not responsible for the accuracy of this information. Converted to HTML by Dionisio@InfiNet.com 11/13/98. (Did a little spell-checking and some minor edits too.) Stolen from  urlLink http://my.ohio.voyager.net/~dionisio/fun/m...own-h-bomb.html  and reformatted the HTML. It now validates to XHTML 1.0 Strict. How to Build an H-Bomb Making and owning an H-bomb is the kind of challenge real Americans seek. Who wants to be a passive victim of nuclear war when, with a little effort, you can be an active participant? Bomb shelters are for losers. Who wants to huddle together underground eating canned Spam? Winners want to push the button themselves. Making your own H-bomb is a big step in nuclear assertiveness training -- it's called Taking Charge. We're sure you'll enjoy the risks and the heady thrill of playing nuclear chicken. Introduction When the Feds clamped down on The Progressive magazine for attempting to publish an article on the manufacture of the hydrogen bomb, it piqued our curiosity. Was it really true that atomic and hydrogen bomb technology was so simple you could build an H-bomb in your own kitchen? Seven Days decided to find out. Food editor Barbara Ehrenreich, investigative reporter Peter Biskind, Photographer Jane Melnick and nuclear scientist Michio Kaku were given three days to cook up a workable H-bomb. They did and we have decided to share their culinary secrets with you. Not that Seven Days supports nuclear terrorism. We don't. We would prefer to die slowly from familiar poisons like low-level radiation, microwaves, DDT, DBCP, aflatoxins, PBBs, PBCs, or food dyes, rather than unexpectedly, say as hostage to a Latvian nationalist brandishing a homemade bomb. In our view the real terrorists are the governments, American, Soviet, French, Chinese, and British, that are hoarding H-bombs for their own use, and worse still, those governments (U.S., French and German) that are eagerly peddling advanced nuclear technology to countries like South Africa, Brazil, and Argentina so that they can make their own bombs. When these bombs are used, and they will be, it will be the world's big-time nuclear peddlers, along with corporate suppliers like General Electric, Westinghouse, and Gulf Oil, that we can thank for it. Gagging The Progressive will do no more for national security than backyard bomb shelters because like it or not the news is out. The heart of the successful H-bomb is the successful A-bomb. Once you've got your A-bombs made the rest is frosting on the cake. All you have to do is set them up so that when they detonate they'll start off a hydrogen-fusion reaction.  Part 1: Making Your Bomb Step 1: Getting the Ingredients Uranium is the basic ingredient of the A-bomb. When a uranium atom's nucleus splits apart, it releases a tremendous amount of energy (for its size), and it emits neutrons which go on to split other nearby uranium nuclei, releasing more energy, in what is called a 'chain reaction'. (When atoms split, matter is converted into energy according to Einstein's equation E=MC2. What better way to mark his birthday than with your own atomic fireworks?) There are two kinds (isotopes) of uranium: the rare U-235, used in bombs, and the more common, heavier, but useless U-238. Natural uranium contains less than 1 percent U-235 and in order to be usable in bombs it has to be 'enriched' to 90 percent U-235 and only 10 percent U-238. Plutonium-239 can also be used in bombs as a substitute for U-235. Ten pounds of U-235 (or slightly less plutonium) is all that is necessary for a bomb. Less than ten pounds won't give you a critical mass. So purifying or enriching naturally occurring uranium is likely to be your first big hurdle. It is infinitely easy to steal ready-to-use enriched uranium or plutonium than to enrich some yourself. And stealing uranium is not as hard as it sounds. There are at least three sources of enriched uranium or plutonium... Enriched uranium is manufactured at a gaseous diffusion plant in Portsmouth, Ohio. From there it is shipped in 10 liter bottles by airplane and trucks to conversion plants that turn it into uranium oxide or uranium metal. Each 10 liter bottle contains 7 kilograms of U-235, and there are 20 bottles to a typical shipment. Conversion facilities exist at Hematite, Missouri; Apollo, Pennsylvania; and Erwin, Tennessee. The Kerr-McGee plant at Crescent Oklahoma -- where Karen Silkwood worked -- was a conversion plant that 'lost' 40 lbs of plutonium. Enriched uranium can be stolen from these plants or from fuel-fabricating plants like those in New Haven, San Diego; or Lynchburg, Virginia. (A former Kerr-McGee supervisor, James V. Smith, when asked at the Silkwood trial if there were any security precautions at the plant to prevent theft, testified that 'There were none of any kind, no guards, no fences, no nothing.') Plutonium can be obtained from places like United Nuclear in Pawling, New York; Nuclear Fuel Services in Erwin, Tennessee; General Electric in Pleasanton, California; Westinghouse in Cheswick, Pennsylvania; Nuclear Materials and Equipment Corporation (NUMEC) in Leechburg, Pennsylvania; and plants in Hanfford, Washington and Morris, Illinois. According to Rolling Stone magazine the Israelis were involved in the theft of plutonium from NUMEC. Finally you can steal enriched uranium or plutonium while it's en-route from conversion plants to fuel fabricating plants. It is usually transported (by air or truck) in the form of uranium oxide, a brownish powder resembling instant coffee, or as a metal, coming in small chunks called 'broken buttons.' Both forms are shipped in small cans stacked in 5-inch cylinders braced with welded struts in the center of ordinary 55 gallon steel drums. The drums weigh about 100 pounds and are clearly marked 'Fissible Material' or 'Danger, Plutonium.' A typical shipment might go from the enrichment plant at Portsmouth, Ohio to the conversion plant in Hematite Missouri then to Kansas City by truck where it would be flown to Los Angeles and then trucked down to the General Atomic plant in San Diego. The plans for the General Atomic plant are on file at the Nuclear Regulatory Commission's reading room at 1717 H Street NW Washington. A Xerox machine is provided for the convenience of the public. If you can't get hold of any enriched uranium you'll have to settle for commercial grade (20 percent U-235). This can be stolen from university reactors of a type called TRIGA Mark II, where security is even more casual than at commercial plants. If stealing uranium seems too tacky you can buy it. Unenriched uranium is available at any chemical supply house for $23 a pound. Commercial grade (3 to 20 percent enriched) is available for $40 a pound from Gulf Atomic. You'll have to enrich it further yourself. Quite frankly this can be something of a pain in the ass. You'll need to start with a little more than 50 pounds of commercial-grade uranium. (It's only 20 percent U-235 at best, and you need 10 pounds of U-235 so... ) But with a little kitchen-table chemistry you'll be able to convert the solid uranium oxide you've purchased into a liquid form. Once you've done that, you'll be able to separate the U-235 that you'll need from the U-238. First pour a few gallons of concentrated hydrofluoric acid into your uranium oxide, converting it to uranium tetrafluoride. (Safety note: Concentrated hydrofluoric acid is so corrosive that it will eat its way through glass, so store it only in plastic. Used 1-gallon plastic milk containers will do.) Now you have to convert your uranium tetrafluoride to uranium hexafluoride, the gaseous form of uranium, which is convenient for separating out the isotope U-235 from U-238. To get the hexafluoride form, bubble fluorine gas into your container of uranium tetrafluoride. Fluorine is available in pressurized tanks from chemical-supply firms. Be careful how you use it though because fluorine is several times more deadly than chlorine, the classic World War I poison gas. Chemists recommend that you carry out this step under a stove hood (the kind used to remove unpleasant cooking odors). If you've done your chemistry right you should now have a generous supply of uranium hexafluoride ready for enriching. In the old horse-and-buggy days of A-bomb manufacture the enrichment was carried out by passing the uranium hexafluoride through hundreds of miles of pipes, tubes, and membranes, until the U-235 was eventually separated from the U-238. This gaseous-diffusion process, as it was called is difficult, time-consuming, and expensive. Gaseous-diffusion plants cover hundreds of acres and cost in the neighborhood of $2-billion each. So forget it. There are easier, and cheaper, ways to enrich your uranium. First transform the gas into a liquid by subjecting it to pressure. You can use a bicycle pump for this. Then make a simple home centrifuge. Fill a standard-size bucket one-quarter full of liquid uranium hexafluoride. Attach a six-foot rope to the bucket handle. Now swing the rope (and attached bucket) around your head as fast as possible. Keep this up for about 45 minutes. Slow down gradually, and very gently put the bucket on the floor. The U-235, which is lighter, will have risen to the top, where it can be skimmed off like cream. Repeat this step until you have the required 10 pounds of uranium. (Safety note: Don't put all your enriched uranium hexafluoride in one bucket. Use at least two or three buckets and keep them in separate corners of the room. This will prevent the premature build-up of a critical mass.) Now it's time to convert your enriched uranium back to metal form. This is easily enough accomplished by spooning several ladlefuls of calcium (available in tablet form from your drugstore) into each bucket of uranium. The calcium will react with the uranium hexafluoride to produce calcium fluoride, a colorless salt which can be easily be separated from your pure enriched uranium metal. A few precautions: • While uranium is not dangerously radioactive in the amounts you'll be handling, if you plan to make more than one bomb it might be wise to wear gloves and a lead apron, the kind you can buy in dental supply stores. • Plutonium is one of the most toxic substances known. If inhaled, a thousandth of a gram can cause massive fibrosis of the lungs, a painful way to go. Even a millionth of a gram in the lungs will cause cancer. If eaten plutonium is metabolized like calcium. It goes straight to the bones where it gives out alpha particles preventing bone marrow from manufacturing red blood cells. The best way to avoid inhaling plutonium is to hold your breath while handling it. If this is too difficult wear a mask. To avoid ingesting plutonium orally follow this simple rule: never make an A-bomb on an empty stomach. • If you find yourself dozing off while you're working, or if you begin to glow in the dark, it might be wise to take a blood count. Prick your finger with a sterile pin, place a drop of blood on a microscope slide, cover it with a cover slip, and examine under a microscope. (Best results are obtained in the early morning.) When you get leukemia, immature cells are released into the bloodstream, and usually the number of white cells increases (though this increase might take almost 2 weeks). Red blood cells look kind of like donuts (without the hole), and are slightly smaller than the white cells, each of which has a nucleus. Immature red cells look similar to white cells (i.e.. slightly larger and have a nucleus). If you have more than about 1 white cell (including immature ones) to 400 red cells then start to worry. But, depending upon your plans for the eventual use of the bomb, a short life expectancy might not be a problem.  Step 2: Assembling the A-Bomb Now that you've acquired the enriched uranium, all that's left is to assemble your A-bomb. Go find a couple of stainless steel salad bowls. You also want to separate your 10 pounds of U-235 into two hunks. (Keep them apart!) The idea is to push each half your uranium into the inside of a bowl. Take one hunk of your uranium and beat it into the inside of the first bowl. Uranium is malleable, like gold, so you should have no trouble hammering it into the bowl to get a good fit. Take another five-pound hunk of uranium and fit it into a second stainless steel bowl. These two bowls of U-235 are the 'subcritical masses' which, when brought together forcefully, will provide the critical mass that makes your A-bomb go. Keep them a respectful distance apart while working because you don't want them to 'go critical' on you... At least not yet. Now hollow out the body of an old vacuum cleaner and place your two hemispherical bowls inside, open ends facing each other, no less than seven inches apart, using masking tape to set them up in position. The reason for the steel bowls and the vacuum cleaner, in case you're wondering, is that these help reflect the neutrons back into the uranium for a more efficient explosion. 'A loose neutron is a useless neutron' as the A-bomb pioneers used to say. As far as the A-bomb goes, you're almost done. The final problem is to figure out how to get the two U-235 hemispheres to smash into each other with sufficient force to set off a truly effective fission reaction. Almost any type of explosive can be used to drive them together. Gunpowder, for example, is easily made at home from potassium nitrate, sulfur, and carbon. Or, you can get some blasting caps or TNT. (Buy them or steal them from a construction site.) Best of all is C4 plastic explosive. You can mold it around your bowls, and it's fairly safe to work with. (But, it might be wise to shape it around an extra salad bowl in another room, and THEN fit it to your uranium-packed bowls. This is particularly true in winter, when a stray static electrical charge might induce ignition in the C4. A responsible bomb maker considers it impolite to accidentally destroy more of the neighborhood than absolutely necessary.) Once the explosives are in place all you need to do is hook up a simple detonation device with a few batteries, a switch, and some wire. Remember though that it is essential that the two charges -- one on each side of the casing -- go off simultaneously. Now put the whole thing in the casing of an old Hoover vacuum cleaner and you're finished with this part of the process. The rest is easy.  Step 3: Make More A-Bombs Following the Directions Above  A Word to the Wise About Wastes After your A-bomb is completed you'll have a pile of moderately fatal radioactive wastes like U-238. These are not dangerous, but you do have to get rid of them. You can flush leftovers down the toilet. (Don't worry about polluting the ocean, there is already so much radioactive waste there, a few more bucketfuls won't make any waves whatsoever.) If you're the fastidious type -- the kind who never leaves gum under their seat at the movies -- you can seal the nasty stuff in coffee cans and bury it in the backyard, just like Uncle Sam does. If the neighbor kids have a habit of trampling the lawn, tell them to play over by the waste. You'll soon find that they're spending most of their time in bed.  Going First Class If you're like us, you're feeling the economic pinch, and you'll want to make your bomb as inexpensively as possible, consonant of course with reasonable yield. The recipe we've given is for a budget-pleasing H-bomb, no frills, no flourishes; it's just a simple 5-megaton bomb, capable of wiping out the New York metropolitan area, the San Francisco Bay area, or Boston. But don't forget, your H-bomb will only be as good as the A-bombs in it. If you want to spend a little more money you can punch-up your A-bomb considerably. Instead of centrifuging your uranium by hand, you can buy a commercial centrifuge. (Fisher Scientific sells one for about $1000.) You also might want to be fussier about your design. The Hiroshima bomb, a relatively crude one, only fissioned 1 percent of it's uranium and yielded only 13 kilotons. In order to fission more of the uranium, the force of your explosive 'trigger' needs to be evenly diffused around the sphere; the same pressure has to be exerted on every point of the sphere simultaneously. (It was a technique for producing this sort of simultaneous detonation by fashioning the explosives into lenses that the government accused Julius and Ethel Rosenberg of trying to steal).  Part 2: Putting Your H-Bomb Together The heart of the H-bomb is the fusion process. Several A-bombs are detonated in such a way as to create the extremely high temperature (100 million degrees C) necessary to fuse lithium deuteride (LiD) into helium. When the lithium nucleus slams into the deuterium nucleus, two helium nuclei are created, and if this happens to enough deuterium nuclei rapidly enough, the result is an enormous amount of energy: the energy of the H-bomb. You don't have to worry about stealing lithium deuteride, it can be purchased from any chemical-supply house. It costs $1000 a pound. If your budget won't allow it you can substitute lithium hydride at $40 a pound. You will need at least 100 pounds. It's a corrosive and toxic powder so be careful. Place the lithium deuteride or hydride in glass jars and surround it with four A-bombs in their casings. Attach them to the same detonator so that they will go off simultaneously. The container for the whole thing is no problem. They can be placed anywhere: Inside an old stereo console, a discarded refrigerator, etc... When the detonator sets off the four A-bombs all eight hemispheres of fissionable material will slam into each other at the same time creating four critical masses and four detonations. This will raise the temperature of the lithium deuteride to 100 million degrees C fast enough (a few billionths of a second) so that the lithium will not be blown all over the neighborhood before the nuclei have time to fuse. The result, at least 1000 times the punch of the puny A-bomb that leveled Hiroshima (20 million tons of TNT vs. 20 thousand tons.)  Part 3: What to do With Your Bomb Now that you have a fully assembled H-bomb housed in an attractive console of your choice you may be wondering, 'What should I do with it?' Every family will have to answer this question according to its own tastes and preferences, but you may want to explore some possibilities which have been successfully pioneered by the American government. 1. Sell Your Bomb and Make a Pile of Money In these days of rising inflation, increasing unemployment, and an uncertain economic outlook, few businesses make as much sense as weapons production. If your career forecast is cloudy, bomb sales may be the only sure way to avoid the humiliation of receiving welfare, or unemployment. Regardless of your present income level, a home H-bomb business can be an invaluable income supplement, and certainly a profitable alternative to selling Tupperware or pirated Girl Scout cookies. Unfortunately for the family bomb business, big government has already cornered a large part of the world market. But this does not mean that there is a shortage of potential customers. The raid on Entebee was the Waterloo of hijacking, and many nationalist groups are now on the alert for new means to get their message across. They'd jump at the chance to get hold of an H-bomb. Emerging nations which can't ante up enough rice or sugar to buy themselves a reactor from G.E. or Westinghouse are also shopping around. You may wonder about the ethics of selling to nations, or groups, whose goals you may disapprove of. But here again, take a tip from our government: forget ideology -- it's cash that counts. And remember, H-bomb sales have a way of escalating, almost like a chain reaction. Suppose you make a sale to South Yemen which you believe to be a Soviet puppet. Well within a few days some discrete inquiries from North Yemen and possibly the Saudis, the Egyptians and the Ethiopians as well can be expected. Similarly, a sale to the IRA will generate a sale to the Ulster government; and a sale to the Tanzanians will bring the Ugandans running, and so forth. It doesn't matter WHICH side you're on, only how many sides there are. Don't forget about the possibility of repeat sales to the same customer. As the experience of both the U.S. and the U.S.S.R. has shown, each individual nation has a potentially infinite need for H-bombs. No customer -- no matter how small -- can ever have too many. 2. Use Your Bomb at Home Many families are attracted to the H-bomb simply as a 'deterrent.' A discrete sticker on the door or on the living room window saying 'This Home Protected by H-bomb' will discourage IRS investigators, census takers, and Jehovah's Witnesses. You'll be surprised how fast the crime rate will go down and property values will go up. And once the news gets out that you are a home H-bomb owner you'll find that you have unexpected leverage in neighborhood disputes over everything from parking places and stereo noise levels to school tax rates. So relax and enjoy the pride and excitement of home H-bomb ownership!  Is It For You? Let's be honest. The H-bomb isn't for everyone. Frankly there are people who can't handle it. They break out in hives at the very mention of mega-death, fallout, or radiation sickness. The following quiz will help you find out whether you have what it takes for home H-bomb ownership. If you can answer 'yes' to six or more of these questions, then you're emotionally eligible to join the nuclear club. If not, a more conventional weapon may be more your cup of tea, try botulism-toxin, laser rays, or nerve gas. 1. I ignore the demands of others. 2. I subscribe to one or more of the following: Soldier of Fortune, Hustler, Popular Mechanics, Self. 3. Though I have many interesting acquaintances, I am my own best friend. 4. I know what to say after you say 'Hello,' but I am seldom interested in pursuing the conversation. 5. I have seen the movie 'The Deer Hunter' more than once. 6. I know that everyone can be a winner if they want to, and I resent whiners. 7. I own one or more of the following: handgun, video game, trash compactor, snowmobile. 8. I am convinced that leukemia is psychosomatic. 9. I am aware that most vegetarians are sexually impotent. 10. I have read evidence that solar energy is a Communist conspiracy.  Myths About Nuclear War Ever since the first mushroom cloud over Hiroshima ushered in the atomic age, a small group of nay-sayers and doom-mongers has lobbied, campaigned and demonstrated to convince Americans that H-bomb ownership, along with nuclear power, is dangerous and unhealthy. Using their virtual stranglehold over the media, these people have tried to discredit everything nuclear from energy to war. They have vastly overrated the risks of nuclear bombs and left many Americans feeling demoralized and indecisive; not sure where the truth lies. Well, here are the myths, and here are the facts. Myth: After a nuclear exchange the earth will no longer be suitable for human habitation. Fact: This is completely false. According to one scientist (quoted in John McPee's The Curve of Binding Energy) 'The largest bomb that has ever been exploded anywhere was 60 megatons, and that is one-thousandth the force of an earthquake, one-thousandth the force of a hurricane. We have lived with earthquakes and hurricanes for a long time.' Another scientist adds, 'It is often assumed that a full blown nuclear war would be the end of life on earth. That is far from the truth. To end life on earth would take at least a thousand times the total yield of all the nuclear explosives existing in the world, and probably a lot more.' Even if humans succumbed, many forms of life would survive a nuclear free-for-all, cockroaches, certain forms of bacteria, and lichens, for instance. Myth: Radiation is bad for you. Fact: Everything is bad for you if you have too much of it. If you eat too many bananas you'll get a stomach-ache. If you get too much sun you can get sunburned (or even skin cancer). Same thing with radiation. Too much may make you feel under the weather, but nuclear industry officials insist that there is no evidence that low-level radiation has any really serious adverse effects. And, high-level radiation may bring unexpected benefits. It speeds up evolution by weeding out unwanted genetic types and creating new ones. (Remember the old saying, 'Two heads are better than one.') Nearer to home, it's plain that radiation will get rid of pesky crab grass and weeds, and teenagers will find that brief exposure to a nuclear burst vaporizes acne and other skin blemishes. (Many survivors of the Hiroshima bomb found that they were free from skin and it's attendant problems forever.) We hope this clears up any misconceptions you may have had. Enjoy your H-Bomb!           \"\n",
      " ...\n",
      " \"                  urlLink     I  felt kind of energetic today . I slept pretty good last night and got up at 8. I wanted to do so much today with the kids, but when the house is a mess I can't go anywhere until it's presentable. The kids really enjoy just going out on the block to ride bikes or play tag with friends, but we never got a chance to get out. Guilt. Well, we're going to Six Flags tomorrow, so that should make up for today!    I cut up another mango this morning. I onlyhave 2 left so I better get shopping. Bill's going tonight so I have to remember to tell him to pick some up. Later on I ate a few almonds and a few dried currants and at around 4pm I had a huge amount of watermelon. I actually feel full without feeling like I ate too much. Cool.  Kait made pasta alfredo for dinner. I will probably have a very small bowl of it with a very big salad. And I only had 2 cigs today. I may have one more after dinner though. Wow, I usually have about 10 a day. This week is going nicely so far.              \"\n",
      " \"                 Wow, I love fruit.  I read that in order to lose weight with raw food, you shouldn't eat stuff with a lot of fat or sweet fruits.  Of course my favorite things are stuff with a lot of fat and sweet fruits.  This morning I started with a banana.  Then I had such a delicious mango a little while later.  I never had mangos growing up.  We were your basic Northern European, Irish, English, Italian family complete with huge amounts of cooked food.  The only raw things I remember eating was an apple and banana here and there, celery (with cream cheese on it) and that bag of nuts at Thanksgiving, and the occasional salad after we moved in with my Italian stepdad.  We thought mangos and coconuts were weird and only saw them in the Spanish bodegas down the street.  Now I can't live without those.   A few hours later we all shared a big bowl of red grapes.  I feel so good after only fruit in the morning.  When I started this new way of eating, I knew I'd have to take it slow in order to stick with it and I decided the only step I will take for now is having only fruit in the morning.  I've done that before with Fit for Life, Somersizing, and Marilu Henner's Total Health Makeover.  I do stay raw usually until dinnertime, so I feel I'm ahead of the game.  Today's lunch and dinner were disastrous so let's not even go there.   I'm drinking mainly spring water or fresh juice now - no more pepsi or iced tea.  And I had 4 cigs today.  Maybe I'll quit before my birthday.  The better I feel, the less I want to smoke.  Tomorrow I'll break out that pilates tape I've been meaning to get to.  I only had 3 hours of sleep last night (because I had coffee at mom's last night so I wouldn't fall asleep driving home and it kept me awake until 5am!)  so I'm off to bed now.        \"\n",
      " \"                 I want to consider myself a rawfoodist.  Yes, I like to make it one word.  I love the whole concept.  I feel I have the gist of it down, but I'm still reading everything I can get my hands on about the subject.  Can changing one's diet like this really change the person?  I want to change.  I haven't been too thrilled with myself lately.  After 3 kids, my weight is up there and I look and feel like hell.  I started adding more and more raw food into my life a couple of weeks ago, but this week is my true beginning.  I am in love with the idea of natural hygiene and living the way I know we're supposed to.   I'm in what they call a 'transitional' phase right now.  I am eating a few complicated meals trying to get the taste and feel of cooked crappy food.   But I do appreciate the simple, mono foods as well.  I think I'm at around 75% raw at the moment with the expectation of getting to 95% in the near future.  The fun part of this whole thing is that I'm tasting new things such as young coconut, dates, and berries that are fresh and organic and not smushed little pathetic plastic-y things found in frozen pancakes.   Let me tell you what I ate today.  I started the morning with coffee.  Ok, I'm down to one cup one or two mornings a week.  Not bad for someone who relied on that thrice daily caffeine jolt just to get some housework done.  Lets pretend I didn't have that.  Anyway, I ate a banana and drove with everyone out to my mom's.  Bill (hubby dearest) took off today to spend some family time.  I actually brought a huge bage of food and my old juicer.   We stopped off at 7-11 where the kids all got a free slurpee (and Bill too)  and I got a great looking, huge salad for $3.99.  At mom's I cut up a nice mango and threw it in the salad (instead of dressing).  That was actually really yummy.  I never thought mango and onions would taste ok together (then Bill reminded me that Bobby Flay made mango salsa once).   Later on I juiced a bunch of carrots with a bit of apple - Kait (10) had some, too.  She loves fresh juice - and makes killer smoothies.  At dinnertime Dad ordered out pizza and salad.  So I had one slice of pizza with a giant sized plate of salad.  Hey, I'm not perfect.  I ended the night with one glass of zinfandel and totalled about 5 cigarettes for the day.  I am quitting this vile habit on my 35th birthday which is in exactly 22 days.   Most days are like this - last week was worse - next week will be better.  I really do love eating raw.  I have a bit of a hard time with peer pressure/will power/power of suggestion - call it what you want.  I'm overcoming that as well.   Here are the current stats:  Weight: 185.5  (I'm 5'4', so this sucks) Bust:  40' Waist:  37' Hips:  43' Upper Arm:  14' Thigh:  22'          \"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in trainData.columns:\n",
    "    print(f\"unique {i} values :\")\n",
    "    print(f\"{trainData[i].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "Csq86i4tJgja",
    "outputId": "c56b878c-fbd2-4349-86ad-490770e085c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td>hey guys - i had the flu today - th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3578</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3627</th>\n",
       "      <td>589736</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Aries</td>\n",
       "      <td>05,August,2004</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679550</th>\n",
       "      <td>3446325</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>Ok, this is very cool. AISO GrepLaw, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679551</th>\n",
       "      <td>3446325</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>Ok, this is very cool. AISO GrepLaw, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679552</th>\n",
       "      <td>3446325</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>Ok, this is very cool. AISO GrepLaw, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679553</th>\n",
       "      <td>3446325</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>Ok, this is very cool. AISO GrepLaw, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679554</th>\n",
       "      <td>3446325</td>\n",
       "      <td>male</td>\n",
       "      <td>24</td>\n",
       "      <td>Advertising</td>\n",
       "      <td>Gemini</td>\n",
       "      <td>25,May,2004</td>\n",
       "      <td>Ok, this is very cool. AISO GrepLaw, De...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4686 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  ...                                               text\n",
       "2310     589736  ...             hey guys - i had the flu today - th...\n",
       "3469     589736  ...                                                   \n",
       "3578     589736  ...                                                   \n",
       "3626     589736  ...                                                   \n",
       "3627     589736  ...                                                   \n",
       "...         ...  ...                                                ...\n",
       "679550  3446325  ...         Ok, this is very cool. AISO GrepLaw, De...\n",
       "679551  3446325  ...         Ok, this is very cool. AISO GrepLaw, De...\n",
       "679552  3446325  ...         Ok, this is very cool. AISO GrepLaw, De...\n",
       "679553  3446325  ...         Ok, this is very cool. AISO GrepLaw, De...\n",
       "679554  3446325  ...         Ok, this is very cool. AISO GrepLaw, De...\n",
       "\n",
       "[4686 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for Duplicate values\n",
    "dups = trainData.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "dup_df = pd.DataFrame(trainData[dups])\n",
    "dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGS5YqT1IVeS",
    "outputId": "3dabc385-d64f-4e6b-d828-d393a8002c87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dropping duplicates (681284, 7)\n",
      "shape after dropping duplicates (676598, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape before dropping duplicates\" , trainData.shape)\n",
    "trainData.drop_duplicates(inplace=True)\n",
    "print(\"shape after dropping duplicates\" , trainData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RJRlkZweUIg"
   },
   "source": [
    "We can observe that id and date not are useful in prediction..lets remove them\n",
    "\n",
    "As data set is very huge...lets work with 100k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKgbUrUWeQxl",
    "outputId": "ad0ddc00-8081-4625-ee88-4a0f59985584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainData (100000, 5)\n"
     ]
    }
   ],
   "source": [
    "trainData.drop(['id','date'], axis=1, inplace=True)\n",
    "trainData = trainData[:100000]\n",
    "print(\"Shape of trainData\", trainData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fm_QiiR0o9Gm",
    "outputId": "6095de3d-23ba-4886-99ca-b4e40f76537d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Last week marked the one year anniversary of the last week of my old job.  I would be lying if I said that I was devastated to leave that job. There are certainly some people I miss working with, but there are also some people I'm glad I don't ever have to deal with again, and the work itself... not so much the fun.  Perhaps it was because I extended myself more than I should have, maybe it was because I answered my work administered cell phone every time it rang, no matter what time it was, but after almost two years of being a company manager I couldn't bear the thought of booking another flight, prepping another apartment or listening to one more upset actor.  I started to crack at the end of May and finally by August I was on my way out. I was so eager to leave at that point that I didn't even have another job lined up and only two months of rent saved.  Probably not the best idea in the Bay Area, but I felt like I had no other choice.  The last show I worked on was one with 'stars'.  It was important for me to keep these stars happy, that was part of my job.  I was to accomplish star bliss by being extremely pleasant, insanely attentive and upholding all of the commitments made by others, many of which I was not aware of.  Most of these promises were made by the Super Powerful Ultimate Director (SPUD) of the company.    The first time I was aware of these clandestine deals was when I booked some business class plane tickets instead of first class. Even though I had the executed contracts in my hands, stating what I could and couldn't do or buy, I received some angry calls from managers in L.A. about my 'out of line' purchases and was told that I was violating verbal agreements which were made by the SPUD  just yesterday .  I was asked more than once if I knew who I was dealing with.  I think I had a pretty good idea.  These people weren't even in town yet and I had knots in my stomach anticipating what else could possibly go wrong.   Then the best part came, I had the pleasure of picking up cast members at the airport.  It was a joy to be driving a filthy company car around and hear of all the promised wonders I was to provide for these stars; the special meals, the tours of the city and the comp tickets I had for them for other shows on their afternoon off. Granted, this was all part of my job and I had no problem fulfilling (or at least delegating) these requests, but it would have been nice if SPUD, who promised all these things, had let me know the specifics or even just said 'hey, I promised all these people so much stuff I don't even  know  what I said, so good luck!'  I ended up looking like a bobblehead on these rides with the stars, smiling and nodding while driving, taking mental notes of what I needed to do. Sleep was lost on these days, rage persisted, so to avoid a break down I had my intern take over for a few airport runs. He too became miffed with the entire experience.  He liked to take his own pristine car to the airport and came back steaming one time because one of the cast members, who seemed to have brief bouts with narcolepsy, ate a crumbly muffin while dozing off in the front seat of his new car.   The show ran for eight weeks, with the cast changing every two weeks, so every other week held new and wonderful surprises for me.  Upon each arrival I could expect the unexpected and nothing was out of the realm of possibility.  I think the worst incident was being obliged to find soul food in Berkeley at 10:00pm on Sunday night, or maybe it was the time I had to find comfortable quarters and a watchful eye for a dog who had just had hip surgery and was on a fair amount of pain killers.  The 12 - 16 hour days I worked during the rehearsal and run of this show were wearing me down.  I was officially miserable.  Something had to be done.  I talked to my boss, who was the  coolest  boss I've ever had, and she said she would see what they could do to compensate me. She told me she would talk to the SPUD and let me know what they come up with.    The next day I was called into the SPUD's office.  The SPUD thanked me for all my hard work over the last few months and assured me that I was doing a bang up job. I sat there patiently while she told me how lucky I was to work with such stars and what a privilege it was for me to be able to get up close and personal with each and every one.  I did my usual routine of nodding and smiling, just like in the car, waiting for some sort of bonus or a mention of some extra vacation time to fly my way, just something more than words.  I had enough of the words, I wanted more.  And more is what I got.  As the SPUD got to the end of her speech she informed me she wanted to give me something to show her gratitude.  SPUD rolled her chair over to her purse and pulled out her personal checkbook.  As she carefully wrote on a check and handed it to me she was proud to announce that she found out my wedding anniversary had recently passed and she knew Alex and I were going camping for the weekend.  This personal check from SPUD was to say thank you for everything.  This personal check from SPUD was to be used to buy a nice bottle of champagne for my anniversary getaway.  This personal check from SPUD had a note that said 'happy anniversary'.  This personal check from SPUD was for the amount of $25.00.  I wasn't sure what to do.  The whole series of events simultaneously confused, irritated and amused me.  I thanked SPUD and left.  I told my boss about it and she chuckled told me that wasn't what she had in mind or discussed with SPUD in terms of compensation, but what could we do?  I shoved the check in my pocket and went home for the day.  I thought about framing the check and putting it above my desk, but then decided to cash it and buy a case of beer and some beef jerky.  A few days later I was sitting at my desk and SPUD came into the office with her son, as she often did on Friday afternoons, to check in with everyone.  SPUD seemed surprised to see me at my desk and asked why I wasn't on my anniversary camping trip.  I explained that I would be leaving after work that day.  SPUD looked out the window at the gloomy cloud cover and the following dialogue took place:  SPUD: Too bad you'll have such shitty weather. me: We're actually going a little further south, so I think it should be okay. SPUD: Oh, well, if it does rain you can just stay in your tent and fuck all weekend! me: what? SPUD: Well, it's true!  Tell Alex I said that's a direct order!  As if this exchange was not unfortunate enough, please keep in mind that there were about seven other people milling around the office to witness it, one of them being SPUD's son.   And that, my friends, was the straw that broke this camel's back, and when I decided I would quit my job.   At least I got a case of beer out of it.         \n",
      "           Meanwhile, headlines from MSNBC.com:   'Pedestrians swarm the streets ... In face of blackout, people cope'  Well, I should hope they cope.  What did they expect?  'In face of blackout, massacre ensues'?  Also wondering what it means, exactly, that pedestrians 'swarm the streets.'  Are they masses of chaotic, swirling dots?  Are they agitated and feeling strongly aggressive?  Are they gathering in larger and larger numbers, helplessly answering a primal drive?  Or perhaps someone threatened the Queen?         \n",
      "        urlLink    We took a ride to the beach on Wednesday.  I ususally avoid going on the really hot days because of the crowds but I figured I'd take the neighbor kids with us and get some sun, salt and sand anyway.&nbsp; urlLink       \n",
      "       Character inside the character, personagem dentro da persona ou mera imagem, just an image, as a wall reflected inside a garden ou numa garagem qualquer - guardem isso como se guarda em celofane um segredo or a mystery.         \n",
      "       I'm feeling not so paranoid today!&nbsp;&nbsp; That wasn't the case earlier, but as the night wears on, I've been feeling better and more relaxed.&nbsp; Perhaps it's the realization that I leave tomorrow for&nbsp; a mini vacation at a cabin up north.  My mind works in weird yet wonderous ways... I got to thinking and I got to thinking after typing that last sentence, &nbsp;what is 'up north'?&nbsp; For someone in Florida heading up north for a vacation that could be just about anywhere.&nbsp; What about someone in Canada?&nbsp; How far up north could you go?  To me, up north is almost akin to a utopia - a place where everyone exisit in peace in seemingly perfect conditions.&nbsp; It's the ideal vacation spot where you will find relaxation and contentment.&nbsp; When things are troubling you, head 'up north'. &nbsp;Though so perfect in all it's makeup, up north also&nbsp;seems almost&nbsp;unreachable and unattainable&nbsp;as well - like a carrot dangled before your eyes.&nbsp; Up north is El Dorado - it's the Fountain of Youth.&nbsp;   Well, regardless of what it is, my daughter and I are heading there this weekend - at least we are going to attempt to find it!&nbsp; Sitting at our campfire with a trillion stars glistening overhead, no phones or computers in sight, I think we'll come darn close.           \n",
      "                  Curiosity killed the cat.             \n",
      "       i licked reb today...i am perpetually happy  dont ask.  anyway today was boring...as the days go on...i learn douchebag is even more of a douche than i originally thought.  he thinks hes intelligent and has a point to what he's saying...but he doesnt...douche.  otherwise nothing hapend today...actaully i gored my hand with a peice of wood and i kept poking pats penis iwth a lightsaber...once more...dont ask.  k thats it...bye           \n",
      "   For those who are opposed to gay marriage, for whatever reason, I'd like to ask a question. It's a simple, innocent one, but its very simplicity may lead you astray. So I need to add caveats, after I ask it. And that question is:     “What is it about gay marriage, which will harm  you ?” \n",
      "          They are a little slow but they get the idea. When I sold my condo a few years ago I told them that they should buy a townhouse and I'll just pay them rent. This way they have a second house that they can make money off of and I can still have a house (I hate paying property organizations like Paradigm).  Of course they just hated this idea, assuming I was trying to pull something over on them (as if) and since I didn't feel like going through the paperwork of getting another house until I figure out where I want to live, I just submitted and rented out this apartment.  Now fast forward to June 27. My mother calls me and we're talking about her trip this week. She's going to be seeing some old friends, one of whom owns four houses in the Charlotte, N.C. area. They rent three of them out to their kids and just bought a fourth for themselves. Of course they've retired early and make great money off the interest rates of the homes while charging their kids pretty minimal rent to stay there. The thinking is that they can help their kids save up and buy houses of their own.  Now all of a sudden my parents think this is a WONDERFUL idea. Just brilliant. They were asleep when I sold my condo and suggested this idea but now thou art awaken.   Sounds all good but I've got six months on my current lease and I'm stuck, plus there's no guarantee I'll want to live here when it's up. So now they want to do this thing fairly quickly (they are impatient/impulsive like that) and I'm stuck in rental hell. I'm so angry I can't even give them a shit-eating 'I told you so' grin.   So there are two options: either a) beg and plead the properties company to let me out of my lease (HA!) or b) Wait and see. The hope is I'll know whether I'm staying here or moving elsewhere by October and we can search for a place. The only saving grace could be if I get this particular job and I'll be forced to move somewhere in between Jacksonville and Atlanta. That could at least solve my lease problems because the paper will just give me the money to buy it out or take over the last six months and put an intern there or something.  Anyway...Parents are frustrating.          \n",
      "        I just read the following post by Anne van Kesteren:  urlLink Channel 9 does a markover and restyles a bit .  Apparently  urlLink Channel 9  at Msdn.com just got a redesign, where they claim to have worked on cross-browser compatibility.  I decided to test the site out in three browsers, namely Opera 7.51, and Firefox 0.9, and Internet Explorer 6, on my screen at 800x600 resolution. Here are the results:   Opera 7.51 - Fine  Firefox 0.9 - Fine  Internet Explorer 6 - Broken   Yes,  broken . And I have screenshots to prove it. I don't know if it is just my version (6.0.2800.1106 to be precise), but the site is in pieces with the boxes not being aligned correctly.  This must be the joke of the decade or something. I still cannot believe it myself. Microsoft actually goes and breaks their pages  for   in  their own browser, while having other browsers display the pages more-or-less correctly.  I think the problem is that the developers have been using a higher screen resolution. However, many people (like myself) are still using 800x600, so you  must  cater for that by testing. Obviously, this testing has not been done propperly.          \n"
     ]
    }
   ],
   "source": [
    "num_list = random.sample(range(len(trainData)), 10)\n",
    "for i in num_list:\n",
    "  print(trainData['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "03kDXCN0dazn",
    "outputId": "ff10f57f-89b6-4121-ba9b-bf155ce9564a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f78caeaa710>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=trainData['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnLDGiRheUeN"
   },
   "source": [
    "From above we can understand most blog authors are in age range of 14 to 36.\n",
    "Very less blog authors of age 40+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "A-NOpwkzklxb",
    "outputId": "ac40e5cf-c100-4dd1-f9a7-0e9ca36d0251"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6, 8))\n",
    "pd.value_counts(trainData['topic']).plot(kind=\"barh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm0Ez7hVnOMY"
   },
   "source": [
    "We can observe highest number of posts are on topics \"indUnk\" followed  by \"student\" and \"Technology\". Very less number of posts on \"Maritime\",\"environment\" and \"construction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpYVizAEqegE"
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def clean_data(string):\n",
    "  \"\"\"\n",
    "  cleaning data before vectorization\n",
    "  \"\"\"\n",
    "  try:    \n",
    "    string = re.sub(r'^http?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n",
    "    words = string.strip().lower().split()    \n",
    "    words = [w for w in words if not w in set(stopwords.words('english')) or not w.isalpha()]\n",
    "    words = [wordnet_lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "  except:\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "PSPOtZDgq5z1",
    "outputId": "64033aef-dc11-4c53-d1ea-ce32292ec0a5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"             So, I've been playing for the last few weeks, and I've been trying to increase my bankroll. I keep a lot of my bankroll off of any individual poker site. Mostly it is in my bank account, cash, or Neteller. However, every time I would break the $1000 barrier online, I'd quickly go on a losing streak and be down around $750-$800. This happened 4 times in 3 weeks. So, this past week, I told myself I wouldn't let it happen again. I decided to play some $20 Sit-'n-Go tourneys. I did pretty well, only managed to win one, but had plenty of second and third place finishes. Before I knew it, I was more than above my bankroll's glass ceiling. So, lets hope this sticks.  I recently read a post online that kind of bothered me. A guy was asking for advice on how to play in a casino for the first time. A gentleman replied that he shouldn't raise too much before the flop since everyone sees the flop with any two cards and will likely make 2 pair to beat your top pair top kicker, or big pair. I couldn't disagree with this more. In a loose game, you need to be raising these hands out, or at least make these chumps put in more 'dead money.' You can't play poker with the defeatist attitude that you're not going to play aggressively because you know a bad beat is going to happen. If you play this way, there is no way you can make a serious long term profit. You're constantly playing with the attitude that something is going to go wrong. You have to be accepting of bad beats, don't get angry. However, you also have to play your cards the best way possible, and that is to make weaker hands pay more to play. Would you not bet your top set if the board was two-suited because a flush might hit? No, that would be insane. The same goes with not raising pre-flop with AK or QQ because people play any Ace or Any King.  So, if you go to play in a casino, don't be a wuss. Raise when you should raise, and if you're in doubt, re-raise. :)  Also, I have no idea if anyone ever reads anything I write. I know one person has, but other than that, have no clue. So, I'd appreciate a comment now and then. Just a 'hey idiot, you suck' would even work.  John          \""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData['text'][56182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "Xiw5yfUTqxRO",
    "outputId": "934fd39b-4cda-40ff-dc9c-e8843a19e67d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'playing last week trying increase bankroll keep lot bankroll individual poker site mostly bank account cash neteller however every time would break barrier online quickly go losing streak around happened time week past week told let happen decided play sit n go tourney pretty well managed win one plenty second third place finish knew bankroll glass ceiling let hope stick recently read post online kind bothered guy asking advice play casino first time gentleman replied raise much flop since everyone see flop two card likely make pair beat top pair top kicker big pair disagree loose game need raising hand least make chump put dead money play poker defeatist attitude going play aggressively know bad beat going happen play way way make serious long term profit constantly playing attitude something going go wrong accepting bad beat get angry however also play card best way possible make weaker hand pay play would bet top set board two suited flush might hit would insane go raising pre flop ak qq people play ace king go play casino wuss raise raise doubt raise also idea anyone ever read anything write know one person clue appreciate comment hey idiot suck would even work john'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(trainData['text'][56182])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "e_bRg0AWsQLk",
    "outputId": "8754f129-f4c9-491e-b939-4c949ab2e722"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"           In het kader van kernfusie op aarde:  MAAK JE EIGEN WATERSTOFBOM   How to build an H-Bomb From: ascott@tartarus.uwa.edu.au (Andrew Scott) Newsgroups: rec.humor Subject: How To Build An H-Bomb (humorous!) Date: 7 Feb 1994 07:41:14 GMT Organization: The University of Western Australia  Original file dated 12th November 1990. Seemed to be a transcript of a 'Seven Days' article. Poorly formatted and corrupted. I have added the text between 'examine under a microscope' and 'malleable, like gold,' as it was missing. If anyone has the full text, please distribute. I am not responsible for the accuracy of this information. Converted to HTML by Dionisio@InfiNet.com 11/13/98. (Did a little spell-checking and some minor edits too.) Stolen from  urlLink http://my.ohio.voyager.net/~dionisio/fun/m...own-h-bomb.html  and reformatted the HTML. It now validates to XHTML 1.0 Strict. How to Build an H-Bomb Making and owning an H-bomb is the kind of challenge real Americans seek. Who wants to be a passive victim of nuclear war when, with a little effort, you can be an active participant? Bomb shelters are for losers. Who wants to huddle together underground eating canned Spam? Winners want to push the button themselves. Making your own H-bomb is a big step in nuclear assertiveness training -- it's called Taking Charge. We're sure you'll enjoy the risks and the heady thrill of playing nuclear chicken. Introduction When the Feds clamped down on The Progressive magazine for attempting to publish an article on the manufacture of the hydrogen bomb, it piqued our curiosity. Was it really true that atomic and hydrogen bomb technology was so simple you could build an H-bomb in your own kitchen? Seven Days decided to find out. Food editor Barbara Ehrenreich, investigative reporter Peter Biskind, Photographer Jane Melnick and nuclear scientist Michio Kaku were given three days to cook up a workable H-bomb. They did and we have decided to share their culinary secrets with you. Not that Seven Days supports nuclear terrorism. We don't. We would prefer to die slowly from familiar poisons like low-level radiation, microwaves, DDT, DBCP, aflatoxins, PBBs, PBCs, or food dyes, rather than unexpectedly, say as hostage to a Latvian nationalist brandishing a homemade bomb. In our view the real terrorists are the governments, American, Soviet, French, Chinese, and British, that are hoarding H-bombs for their own use, and worse still, those governments (U.S., French and German) that are eagerly peddling advanced nuclear technology to countries like South Africa, Brazil, and Argentina so that they can make their own bombs. When these bombs are used, and they will be, it will be the world's big-time nuclear peddlers, along with corporate suppliers like General Electric, Westinghouse, and Gulf Oil, that we can thank for it. Gagging The Progressive will do no more for national security than backyard bomb shelters because like it or not the news is out. The heart of the successful H-bomb is the successful A-bomb. Once you've got your A-bombs made the rest is frosting on the cake. All you have to do is set them up so that when they detonate they'll start off a hydrogen-fusion reaction.  Part 1: Making Your Bomb Step 1: Getting the Ingredients Uranium is the basic ingredient of the A-bomb. When a uranium atom's nucleus splits apart, it releases a tremendous amount of energy (for its size), and it emits neutrons which go on to split other nearby uranium nuclei, releasing more energy, in what is called a 'chain reaction'. (When atoms split, matter is converted into energy according to Einstein's equation E=MC2. What better way to mark his birthday than with your own atomic fireworks?) There are two kinds (isotopes) of uranium: the rare U-235, used in bombs, and the more common, heavier, but useless U-238. Natural uranium contains less than 1 percent U-235 and in order to be usable in bombs it has to be 'enriched' to 90 percent U-235 and only 10 percent U-238. Plutonium-239 can also be used in bombs as a substitute for U-235. Ten pounds of U-235 (or slightly less plutonium) is all that is necessary for a bomb. Less than ten pounds won't give you a critical mass. So purifying or enriching naturally occurring uranium is likely to be your first big hurdle. It is infinitely easy to steal ready-to-use enriched uranium or plutonium than to enrich some yourself. And stealing uranium is not as hard as it sounds. There are at least three sources of enriched uranium or plutonium... Enriched uranium is manufactured at a gaseous diffusion plant in Portsmouth, Ohio. From there it is shipped in 10 liter bottles by airplane and trucks to conversion plants that turn it into uranium oxide or uranium metal. Each 10 liter bottle contains 7 kilograms of U-235, and there are 20 bottles to a typical shipment. Conversion facilities exist at Hematite, Missouri; Apollo, Pennsylvania; and Erwin, Tennessee. The Kerr-McGee plant at Crescent Oklahoma -- where Karen Silkwood worked -- was a conversion plant that 'lost' 40 lbs of plutonium. Enriched uranium can be stolen from these plants or from fuel-fabricating plants like those in New Haven, San Diego; or Lynchburg, Virginia. (A former Kerr-McGee supervisor, James V. Smith, when asked at the Silkwood trial if there were any security precautions at the plant to prevent theft, testified that 'There were none of any kind, no guards, no fences, no nothing.') Plutonium can be obtained from places like United Nuclear in Pawling, New York; Nuclear Fuel Services in Erwin, Tennessee; General Electric in Pleasanton, California; Westinghouse in Cheswick, Pennsylvania; Nuclear Materials and Equipment Corporation (NUMEC) in Leechburg, Pennsylvania; and plants in Hanfford, Washington and Morris, Illinois. According to Rolling Stone magazine the Israelis were involved in the theft of plutonium from NUMEC. Finally you can steal enriched uranium or plutonium while it's en-route from conversion plants to fuel fabricating plants. It is usually transported (by air or truck) in the form of uranium oxide, a brownish powder resembling instant coffee, or as a metal, coming in small chunks called 'broken buttons.' Both forms are shipped in small cans stacked in 5-inch cylinders braced with welded struts in the center of ordinary 55 gallon steel drums. The drums weigh about 100 pounds and are clearly marked 'Fissible Material' or 'Danger, Plutonium.' A typical shipment might go from the enrichment plant at Portsmouth, Ohio to the conversion plant in Hematite Missouri then to Kansas City by truck where it would be flown to Los Angeles and then trucked down to the General Atomic plant in San Diego. The plans for the General Atomic plant are on file at the Nuclear Regulatory Commission's reading room at 1717 H Street NW Washington. A Xerox machine is provided for the convenience of the public. If you can't get hold of any enriched uranium you'll have to settle for commercial grade (20 percent U-235). This can be stolen from university reactors of a type called TRIGA Mark II, where security is even more casual than at commercial plants. If stealing uranium seems too tacky you can buy it. Unenriched uranium is available at any chemical supply house for $23 a pound. Commercial grade (3 to 20 percent enriched) is available for $40 a pound from Gulf Atomic. You'll have to enrich it further yourself. Quite frankly this can be something of a pain in the ass. You'll need to start with a little more than 50 pounds of commercial-grade uranium. (It's only 20 percent U-235 at best, and you need 10 pounds of U-235 so... ) But with a little kitchen-table chemistry you'll be able to convert the solid uranium oxide you've purchased into a liquid form. Once you've done that, you'll be able to separate the U-235 that you'll need from the U-238. First pour a few gallons of concentrated hydrofluoric acid into your uranium oxide, converting it to uranium tetrafluoride. (Safety note: Concentrated hydrofluoric acid is so corrosive that it will eat its way through glass, so store it only in plastic. Used 1-gallon plastic milk containers will do.) Now you have to convert your uranium tetrafluoride to uranium hexafluoride, the gaseous form of uranium, which is convenient for separating out the isotope U-235 from U-238. To get the hexafluoride form, bubble fluorine gas into your container of uranium tetrafluoride. Fluorine is available in pressurized tanks from chemical-supply firms. Be careful how you use it though because fluorine is several times more deadly than chlorine, the classic World War I poison gas. Chemists recommend that you carry out this step under a stove hood (the kind used to remove unpleasant cooking odors). If you've done your chemistry right you should now have a generous supply of uranium hexafluoride ready for enriching. In the old horse-and-buggy days of A-bomb manufacture the enrichment was carried out by passing the uranium hexafluoride through hundreds of miles of pipes, tubes, and membranes, until the U-235 was eventually separated from the U-238. This gaseous-diffusion process, as it was called is difficult, time-consuming, and expensive. Gaseous-diffusion plants cover hundreds of acres and cost in the neighborhood of $2-billion each. So forget it. There are easier, and cheaper, ways to enrich your uranium. First transform the gas into a liquid by subjecting it to pressure. You can use a bicycle pump for this. Then make a simple home centrifuge. Fill a standard-size bucket one-quarter full of liquid uranium hexafluoride. Attach a six-foot rope to the bucket handle. Now swing the rope (and attached bucket) around your head as fast as possible. Keep this up for about 45 minutes. Slow down gradually, and very gently put the bucket on the floor. The U-235, which is lighter, will have risen to the top, where it can be skimmed off like cream. Repeat this step until you have the required 10 pounds of uranium. (Safety note: Don't put all your enriched uranium hexafluoride in one bucket. Use at least two or three buckets and keep them in separate corners of the room. This will prevent the premature build-up of a critical mass.) Now it's time to convert your enriched uranium back to metal form. This is easily enough accomplished by spooning several ladlefuls of calcium (available in tablet form from your drugstore) into each bucket of uranium. The calcium will react with the uranium hexafluoride to produce calcium fluoride, a colorless salt which can be easily be separated from your pure enriched uranium metal. A few precautions: • While uranium is not dangerously radioactive in the amounts you'll be handling, if you plan to make more than one bomb it might be wise to wear gloves and a lead apron, the kind you can buy in dental supply stores. • Plutonium is one of the most toxic substances known. If inhaled, a thousandth of a gram can cause massive fibrosis of the lungs, a painful way to go. Even a millionth of a gram in the lungs will cause cancer. If eaten plutonium is metabolized like calcium. It goes straight to the bones where it gives out alpha particles preventing bone marrow from manufacturing red blood cells. The best way to avoid inhaling plutonium is to hold your breath while handling it. If this is too difficult wear a mask. To avoid ingesting plutonium orally follow this simple rule: never make an A-bomb on an empty stomach. • If you find yourself dozing off while you're working, or if you begin to glow in the dark, it might be wise to take a blood count. Prick your finger with a sterile pin, place a drop of blood on a microscope slide, cover it with a cover slip, and examine under a microscope. (Best results are obtained in the early morning.) When you get leukemia, immature cells are released into the bloodstream, and usually the number of white cells increases (though this increase might take almost 2 weeks). Red blood cells look kind of like donuts (without the hole), and are slightly smaller than the white cells, each of which has a nucleus. Immature red cells look similar to white cells (i.e.. slightly larger and have a nucleus). If you have more than about 1 white cell (including immature ones) to 400 red cells then start to worry. But, depending upon your plans for the eventual use of the bomb, a short life expectancy might not be a problem.  Step 2: Assembling the A-Bomb Now that you've acquired the enriched uranium, all that's left is to assemble your A-bomb. Go find a couple of stainless steel salad bowls. You also want to separate your 10 pounds of U-235 into two hunks. (Keep them apart!) The idea is to push each half your uranium into the inside of a bowl. Take one hunk of your uranium and beat it into the inside of the first bowl. Uranium is malleable, like gold, so you should have no trouble hammering it into the bowl to get a good fit. Take another five-pound hunk of uranium and fit it into a second stainless steel bowl. These two bowls of U-235 are the 'subcritical masses' which, when brought together forcefully, will provide the critical mass that makes your A-bomb go. Keep them a respectful distance apart while working because you don't want them to 'go critical' on you... At least not yet. Now hollow out the body of an old vacuum cleaner and place your two hemispherical bowls inside, open ends facing each other, no less than seven inches apart, using masking tape to set them up in position. The reason for the steel bowls and the vacuum cleaner, in case you're wondering, is that these help reflect the neutrons back into the uranium for a more efficient explosion. 'A loose neutron is a useless neutron' as the A-bomb pioneers used to say. As far as the A-bomb goes, you're almost done. The final problem is to figure out how to get the two U-235 hemispheres to smash into each other with sufficient force to set off a truly effective fission reaction. Almost any type of explosive can be used to drive them together. Gunpowder, for example, is easily made at home from potassium nitrate, sulfur, and carbon. Or, you can get some blasting caps or TNT. (Buy them or steal them from a construction site.) Best of all is C4 plastic explosive. You can mold it around your bowls, and it's fairly safe to work with. (But, it might be wise to shape it around an extra salad bowl in another room, and THEN fit it to your uranium-packed bowls. This is particularly true in winter, when a stray static electrical charge might induce ignition in the C4. A responsible bomb maker considers it impolite to accidentally destroy more of the neighborhood than absolutely necessary.) Once the explosives are in place all you need to do is hook up a simple detonation device with a few batteries, a switch, and some wire. Remember though that it is essential that the two charges -- one on each side of the casing -- go off simultaneously. Now put the whole thing in the casing of an old Hoover vacuum cleaner and you're finished with this part of the process. The rest is easy.  Step 3: Make More A-Bombs Following the Directions Above  A Word to the Wise About Wastes After your A-bomb is completed you'll have a pile of moderately fatal radioactive wastes like U-238. These are not dangerous, but you do have to get rid of them. You can flush leftovers down the toilet. (Don't worry about polluting the ocean, there is already so much radioactive waste there, a few more bucketfuls won't make any waves whatsoever.) If you're the fastidious type -- the kind who never leaves gum under their seat at the movies -- you can seal the nasty stuff in coffee cans and bury it in the backyard, just like Uncle Sam does. If the neighbor kids have a habit of trampling the lawn, tell them to play over by the waste. You'll soon find that they're spending most of their time in bed.  Going First Class If you're like us, you're feeling the economic pinch, and you'll want to make your bomb as inexpensively as possible, consonant of course with reasonable yield. The recipe we've given is for a budget-pleasing H-bomb, no frills, no flourishes; it's just a simple 5-megaton bomb, capable of wiping out the New York metropolitan area, the San Francisco Bay area, or Boston. But don't forget, your H-bomb will only be as good as the A-bombs in it. If you want to spend a little more money you can punch-up your A-bomb considerably. Instead of centrifuging your uranium by hand, you can buy a commercial centrifuge. (Fisher Scientific sells one for about $1000.) You also might want to be fussier about your design. The Hiroshima bomb, a relatively crude one, only fissioned 1 percent of it's uranium and yielded only 13 kilotons. In order to fission more of the uranium, the force of your explosive 'trigger' needs to be evenly diffused around the sphere; the same pressure has to be exerted on every point of the sphere simultaneously. (It was a technique for producing this sort of simultaneous detonation by fashioning the explosives into lenses that the government accused Julius and Ethel Rosenberg of trying to steal).  Part 2: Putting Your H-Bomb Together The heart of the H-bomb is the fusion process. Several A-bombs are detonated in such a way as to create the extremely high temperature (100 million degrees C) necessary to fuse lithium deuteride (LiD) into helium. When the lithium nucleus slams into the deuterium nucleus, two helium nuclei are created, and if this happens to enough deuterium nuclei rapidly enough, the result is an enormous amount of energy: the energy of the H-bomb. You don't have to worry about stealing lithium deuteride, it can be purchased from any chemical-supply house. It costs $1000 a pound. If your budget won't allow it you can substitute lithium hydride at $40 a pound. You will need at least 100 pounds. It's a corrosive and toxic powder so be careful. Place the lithium deuteride or hydride in glass jars and surround it with four A-bombs in their casings. Attach them to the same detonator so that they will go off simultaneously. The container for the whole thing is no problem. They can be placed anywhere: Inside an old stereo console, a discarded refrigerator, etc... When the detonator sets off the four A-bombs all eight hemispheres of fissionable material will slam into each other at the same time creating four critical masses and four detonations. This will raise the temperature of the lithium deuteride to 100 million degrees C fast enough (a few billionths of a second) so that the lithium will not be blown all over the neighborhood before the nuclei have time to fuse. The result, at least 1000 times the punch of the puny A-bomb that leveled Hiroshima (20 million tons of TNT vs. 20 thousand tons.)  Part 3: What to do With Your Bomb Now that you have a fully assembled H-bomb housed in an attractive console of your choice you may be wondering, 'What should I do with it?' Every family will have to answer this question according to its own tastes and preferences, but you may want to explore some possibilities which have been successfully pioneered by the American government. 1. Sell Your Bomb and Make a Pile of Money In these days of rising inflation, increasing unemployment, and an uncertain economic outlook, few businesses make as much sense as weapons production. If your career forecast is cloudy, bomb sales may be the only sure way to avoid the humiliation of receiving welfare, or unemployment. Regardless of your present income level, a home H-bomb business can be an invaluable income supplement, and certainly a profitable alternative to selling Tupperware or pirated Girl Scout cookies. Unfortunately for the family bomb business, big government has already cornered a large part of the world market. But this does not mean that there is a shortage of potential customers. The raid on Entebee was the Waterloo of hijacking, and many nationalist groups are now on the alert for new means to get their message across. They'd jump at the chance to get hold of an H-bomb. Emerging nations which can't ante up enough rice or sugar to buy themselves a reactor from G.E. or Westinghouse are also shopping around. You may wonder about the ethics of selling to nations, or groups, whose goals you may disapprove of. But here again, take a tip from our government: forget ideology -- it's cash that counts. And remember, H-bomb sales have a way of escalating, almost like a chain reaction. Suppose you make a sale to South Yemen which you believe to be a Soviet puppet. Well within a few days some discrete inquiries from North Yemen and possibly the Saudis, the Egyptians and the Ethiopians as well can be expected. Similarly, a sale to the IRA will generate a sale to the Ulster government; and a sale to the Tanzanians will bring the Ugandans running, and so forth. It doesn't matter WHICH side you're on, only how many sides there are. Don't forget about the possibility of repeat sales to the same customer. As the experience of both the U.S. and the U.S.S.R. has shown, each individual nation has a potentially infinite need for H-bombs. No customer -- no matter how small -- can ever have too many. 2. Use Your Bomb at Home Many families are attracted to the H-bomb simply as a 'deterrent.' A discrete sticker on the door or on the living room window saying 'This Home Protected by H-bomb' will discourage IRS investigators, census takers, and Jehovah's Witnesses. You'll be surprised how fast the crime rate will go down and property values will go up. And once the news gets out that you are a home H-bomb owner you'll find that you have unexpected leverage in neighborhood disputes over everything from parking places and stereo noise levels to school tax rates. So relax and enjoy the pride and excitement of home H-bomb ownership!  Is It For You? Let's be honest. The H-bomb isn't for everyone. Frankly there are people who can't handle it. They break out in hives at the very mention of mega-death, fallout, or radiation sickness. The following quiz will help you find out whether you have what it takes for home H-bomb ownership. If you can answer 'yes' to six or more of these questions, then you're emotionally eligible to join the nuclear club. If not, a more conventional weapon may be more your cup of tea, try botulism-toxin, laser rays, or nerve gas. 1. I ignore the demands of others. 2. I subscribe to one or more of the following: Soldier of Fortune, Hustler, Popular Mechanics, Self. 3. Though I have many interesting acquaintances, I am my own best friend. 4. I know what to say after you say 'Hello,' but I am seldom interested in pursuing the conversation. 5. I have seen the movie 'The Deer Hunter' more than once. 6. I know that everyone can be a winner if they want to, and I resent whiners. 7. I own one or more of the following: handgun, video game, trash compactor, snowmobile. 8. I am convinced that leukemia is psychosomatic. 9. I am aware that most vegetarians are sexually impotent. 10. I have read evidence that solar energy is a Communist conspiracy.  Myths About Nuclear War Ever since the first mushroom cloud over Hiroshima ushered in the atomic age, a small group of nay-sayers and doom-mongers has lobbied, campaigned and demonstrated to convince Americans that H-bomb ownership, along with nuclear power, is dangerous and unhealthy. Using their virtual stranglehold over the media, these people have tried to discredit everything nuclear from energy to war. They have vastly overrated the risks of nuclear bombs and left many Americans feeling demoralized and indecisive; not sure where the truth lies. Well, here are the myths, and here are the facts. Myth: After a nuclear exchange the earth will no longer be suitable for human habitation. Fact: This is completely false. According to one scientist (quoted in John McPee's The Curve of Binding Energy) 'The largest bomb that has ever been exploded anywhere was 60 megatons, and that is one-thousandth the force of an earthquake, one-thousandth the force of a hurricane. We have lived with earthquakes and hurricanes for a long time.' Another scientist adds, 'It is often assumed that a full blown nuclear war would be the end of life on earth. That is far from the truth. To end life on earth would take at least a thousand times the total yield of all the nuclear explosives existing in the world, and probably a lot more.' Even if humans succumbed, many forms of life would survive a nuclear free-for-all, cockroaches, certain forms of bacteria, and lichens, for instance. Myth: Radiation is bad for you. Fact: Everything is bad for you if you have too much of it. If you eat too many bananas you'll get a stomach-ache. If you get too much sun you can get sunburned (or even skin cancer). Same thing with radiation. Too much may make you feel under the weather, but nuclear industry officials insist that there is no evidence that low-level radiation has any really serious adverse effects. And, high-level radiation may bring unexpected benefits. It speeds up evolution by weeding out unwanted genetic types and creating new ones. (Remember the old saying, 'Two heads are better than one.') Nearer to home, it's plain that radiation will get rid of pesky crab grass and weeds, and teenagers will find that brief exposure to a nuclear burst vaporizes acne and other skin blemishes. (Many survivors of the Hiroshima bomb found that they were free from skin and it's attendant problems forever.) We hope this clears up any misconceptions you may have had. Enjoy your H-Bomb!           \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "g1GD2gcxsgPh",
    "outputId": "8923848b-6016-439f-ef4f-fdfde613514b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'het kader van kernfusie op aarde maak je eigen waterstofbom build h bomb ascott tartarus uwa edu au andrew scott newsgroups rec humor subject build h bomb humorous date feb gmt organization university western australia original file dated th november seemed transcript seven day article poorly formatted corrupted added text examine microscope malleable like gold missing anyone full text please distribute responsible accuracy information converted html dionisio infinet com little spell checking minor edits stolen urllink http ohio voyager net dionisio fun h bomb html reformatted html validates xhtml strict build h bomb making owning h bomb kind challenge real american seek want passive victim nuclear war little effort active participant bomb shelter loser want huddle together underground eating canned spam winner want push button making h bomb big step nuclear assertiveness training called taking charge sure enjoy risk heady thrill playing nuclear chicken introduction fed clamped progressive magazine attempting publish article manufacture hydrogen bomb piqued curiosity really true atomic hydrogen bomb technology simple could build h bomb kitchen seven day decided find food editor barbara ehrenreich investigative reporter peter biskind photographer jane melnick nuclear scientist michio kaku given three day cook workable h bomb decided share culinary secret seven day support nuclear terrorism would prefer die slowly familiar poison like low level radiation microwave ddt dbcp aflatoxin pbbs pbcs food dye rather unexpectedly say hostage latvian nationalist brandishing homemade bomb view real terrorist government american soviet french chinese british hoarding h bomb use worse still government u french german eagerly peddling advanced nuclear technology country like south africa brazil argentina make bomb bomb used world big time nuclear peddler along corporate supplier like general electric westinghouse gulf oil thank gagging progressive national security backyard bomb shelter like news heart successful h bomb successful bomb got bomb made rest frosting cake set detonate start hydrogen fusion reaction part making bomb step getting ingredient uranium basic ingredient bomb uranium atom nucleus split apart release tremendous amount energy size emits neutron go split nearby uranium nucleus releasing energy called chain reaction atom split matter converted energy according einstein equation e mc better way mark birthday atomic firework two kind isotope uranium rare u used bomb common heavier useless u natural uranium contains le percent u order usable bomb enriched percent u percent u plutonium also used bomb substitute u ten pound u slightly le plutonium necessary bomb le ten pound give critical mass purifying enriching naturally occurring uranium likely first big hurdle infinitely easy steal ready use enriched uranium plutonium enrich stealing uranium hard sound least three source enriched uranium plutonium enriched uranium manufactured gaseous diffusion plant portsmouth ohio shipped liter bottle airplane truck conversion plant turn uranium oxide uranium metal liter bottle contains kilogram u bottle typical shipment conversion facility exist hematite missouri apollo pennsylvania erwin tennessee kerr mcgee plant crescent oklahoma karen silkwood worked conversion plant lost lb plutonium enriched uranium stolen plant fuel fabricating plant like new san diego lynchburg virginia former kerr mcgee supervisor james v smith asked silkwood trial security precaution plant prevent theft testified none kind guard fence nothing plutonium obtained place like united nuclear pawling new york nuclear fuel service erwin tennessee general electric pleasanton california westinghouse cheswick pennsylvania nuclear material equipment corporation numec leechburg pennsylvania plant hanfford washington morris illinois according rolling stone magazine israeli involved theft plutonium numec finally steal enriched uranium plutonium en route conversion plant fuel fabricating plant usually transported air truck form uranium oxide brownish powder resembling instant coffee metal coming small chunk called broken button form shipped small can stacked inch cylinder braced welded strut center ordinary gallon steel drum drum weigh pound clearly marked fissible material danger plutonium typical shipment might go enrichment plant portsmouth ohio conversion plant hematite missouri kansa city truck would flown los angeles trucked general atomic plant san diego plan general atomic plant file nuclear regulatory commission reading room h street nw washington xerox machine provided convenience public get hold enriched uranium settle commercial grade percent u stolen university reactor type called triga mark ii security even casual commercial plant stealing uranium seems tacky buy unenriched uranium available chemical supply house pound commercial grade percent enriched available pound gulf atomic enrich quite frankly something pain as need start little pound commercial grade uranium percent u best need pound u little kitchen table chemistry able convert solid uranium oxide purchased liquid form done able separate u need u first pour gallon concentrated hydrofluoric acid uranium oxide converting uranium tetrafluoride safety note concentrated hydrofluoric acid corrosive eat way glass store plastic used gallon plastic milk container convert uranium tetrafluoride uranium hexafluoride gaseous form uranium convenient separating isotope u u get hexafluoride form bubble fluorine gas container uranium tetrafluoride fluorine available pressurized tank chemical supply firm careful use though fluorine several time deadly chlorine classic world war poison gas chemist recommend carry step stove hood kind used remove unpleasant cooking odor done chemistry right generous supply uranium hexafluoride ready enriching old horse buggy day bomb manufacture enrichment carried passing uranium hexafluoride hundred mile pipe tube membrane u eventually separated u gaseous diffusion process called difficult time consuming expensive gaseous diffusion plant cover hundred acre cost neighborhood billion forget easier cheaper way enrich uranium first transform gas liquid subjecting pressure use bicycle pump make simple home centrifuge fill standard size bucket one quarter full liquid uranium hexafluoride attach six foot rope bucket handle swing rope attached bucket around head fast possible keep minute slow gradually gently put bucket floor u lighter risen top skimmed like cream repeat step required pound uranium safety note put enriched uranium hexafluoride one bucket use least two three bucket keep separate corner room prevent premature build critical mass time convert enriched uranium back metal form easily enough accomplished spooning several ladlefuls calcium available tablet form drugstore bucket uranium calcium react uranium hexafluoride produce calcium fluoride colorless salt easily separated pure enriched uranium metal precaution uranium dangerously radioactive amount handling plan make one bomb might wise wear glove lead apron kind buy dental supply store plutonium one toxic substance known inhaled thousandth gram cause massive fibrosis lung painful way go even millionth gram lung cause cancer eaten plutonium metabolized like calcium go straight bone give alpha particle preventing bone marrow manufacturing red blood cell best way avoid inhaling plutonium hold breath handling difficult wear mask avoid ingesting plutonium orally follow simple rule never make bomb empty stomach find dozing working begin glow dark might wise take blood count prick finger sterile pin place drop blood microscope slide cover cover slip examine microscope best result obtained early morning get leukemia immature cell released bloodstream usually number white cell increase though increase might take almost week red blood cell look kind like donut without hole slightly smaller white cell nucleus immature red cell look similar white cell e slightly larger nucleus white cell including immature one red cell start worry depending upon plan eventual use bomb short life expectancy might problem step assembling bomb acquired enriched uranium left assemble bomb go find couple stainless steel salad bowl also want separate pound u two hunk keep apart idea push half uranium inside bowl take one hunk uranium beat inside first bowl uranium malleable like gold trouble hammering bowl get good fit take another five pound hunk uranium fit second stainless steel bowl two bowl u subcritical mass brought together forcefully provide critical mass make bomb go keep respectful distance apart working want go critical least yet hollow body old vacuum cleaner place two hemispherical bowl inside open end facing le seven inch apart using masking tape set position reason steel bowl vacuum cleaner case wondering help reflect neutron back uranium efficient explosion loose neutron useless neutron bomb pioneer used say far bomb go almost done final problem figure get two u hemisphere smash sufficient force set truly effective fission reaction almost type explosive used drive together gunpowder example easily made home potassium nitrate sulfur carbon get blasting cap tnt buy steal construction site best c plastic explosive mold around bowl fairly safe work might wise shape around extra salad bowl another room fit uranium packed bowl particularly true winter stray static electrical charge might induce ignition c responsible bomb maker considers impolite accidentally destroy neighborhood absolutely necessary explosive place need hook simple detonation device battery switch wire remember though essential two charge one side casing go simultaneously put whole thing casing old hoover vacuum cleaner finished part process rest easy step make bomb following direction word wise waste bomb completed pile moderately fatal radioactive waste like u dangerous get rid flush leftover toilet worry polluting ocean already much radioactive waste bucketful make wave whatsoever fastidious type kind never leaf gum seat movie seal nasty stuff coffee can bury backyard like uncle sam neighbor kid habit trampling lawn tell play waste soon find spending time bed going first class like u feeling economic pinch want make bomb inexpensively possible consonant course reasonable yield recipe given budget pleasing h bomb frill flourish simple megaton bomb capable wiping new york metropolitan area san francisco bay area boston forget h bomb good bomb want spend little money punch bomb considerably instead centrifuging uranium hand buy commercial centrifuge fisher scientific sell one also might want fussier design hiroshima bomb relatively crude one fissioned percent uranium yielded kiloton order fission uranium force explosive trigger need evenly diffused around sphere pressure exerted every point sphere simultaneously technique producing sort simultaneous detonation fashioning explosive lens government accused julius ethel rosenberg trying steal part putting h bomb together heart h bomb fusion process several bomb detonated way create extremely high temperature million degree c necessary fuse lithium deuteride lid helium lithium nucleus slam deuterium nucleus two helium nucleus created happens enough deuterium nucleus rapidly enough result enormous amount energy energy h bomb worry stealing lithium deuteride purchased chemical supply house cost pound budget allow substitute lithium hydride pound need least pound corrosive toxic powder careful place lithium deuteride hydride glass jar surround four bomb casing attach detonator go simultaneously container whole thing problem placed anywhere inside old stereo console discarded refrigerator etc detonator set four bomb eight hemisphere fissionable material slam time creating four critical mass four detonation raise temperature lithium deuteride million degree c fast enough billionth second lithium blown neighborhood nucleus time fuse result least time punch puny bomb leveled hiroshima million ton tnt v thousand ton part bomb fully assembled h bomb housed attractive console choice may wondering every family answer question according taste preference may want explore possibility successfully pioneered american government sell bomb make pile money day rising inflation increasing unemployment uncertain economic outlook business make much sense weapon production career forecast cloudy bomb sale may sure way avoid humiliation receiving welfare unemployment regardless present income level home h bomb business invaluable income supplement certainly profitable alternative selling tupperware pirated girl scout cooky unfortunately family bomb business big government already cornered large part world market mean shortage potential customer raid entebee waterloo hijacking many nationalist group alert new mean get message across jump chance get hold h bomb emerging nation ante enough rice sugar buy reactor g e westinghouse also shopping around may wonder ethic selling nation group whose goal may disapprove take tip government forget ideology cash count remember h bomb sale way escalating almost like chain reaction suppose make sale south yemen believe soviet puppet well within day discrete inquiry north yemen possibly saudi egyptian ethiopian well expected similarly sale ira generate sale ulster government sale tanzanian bring ugandan running forth matter side many side forget possibility repeat sale customer experience u u r shown individual nation potentially infinite need h bomb customer matter small ever many use bomb home many family attracted h bomb simply deterrent discrete sticker door living room window saying home protected h bomb discourage irs investigator census taker jehovah witness surprised fast crime rate go property value go news get home h bomb owner find unexpected leverage neighborhood dispute everything parking place stereo noise level school tax rate relax enjoy pride excitement home h bomb ownership let honest h bomb everyone frankly people handle break hive mention mega death fallout radiation sickness following quiz help find whether take home h bomb ownership answer yes six question emotionally eligible join nuclear club conventional weapon may cup tea try botulism toxin laser ray nerve gas ignore demand others subscribe one following soldier fortune hustler popular mechanic self though many interesting acquaintance best friend know say say hello seldom interested pursuing conversation seen movie deer hunter know everyone winner want resent whiner one following handgun video game trash compactor snowmobile convinced leukemia psychosomatic aware vegetarian sexually impotent read evidence solar energy communist conspiracy myth nuclear war ever since first mushroom cloud hiroshima ushered atomic age small group nay sayers doom monger lobbied campaigned demonstrated convince american h bomb ownership along nuclear power dangerous unhealthy using virtual stranglehold medium people tried discredit everything nuclear energy war vastly overrated risk nuclear bomb left many american feeling demoralized indecisive sure truth lie well myth fact myth nuclear exchange earth longer suitable human habitation fact completely false according one scientist quoted john mcpee curve binding energy largest bomb ever exploded anywhere megaton one thousandth force earthquake one thousandth force hurricane lived earthquake hurricane long time another scientist add often assumed full blown nuclear war would end life earth far truth end life earth would take least thousand time total yield nuclear explosive existing world probably lot even human succumbed many form life would survive nuclear free cockroach certain form bacteria lichen instance myth radiation bad fact everything bad much eat many banana get stomach ache get much sun get sunburned even skin cancer thing radiation much may make feel weather nuclear industry official insist evidence low level radiation really serious adverse effect high level radiation may bring unexpected benefit speed evolution weeding unwanted genetic type creating new one remember old saying two head better one nearer home plain radiation get rid pesky crab grass weed teenager find brief exposure nuclear burst vaporizes acne skin blemish many survivor hiroshima bomb found free skin attendant problem forever hope clear misconception may enjoy h bomb'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(trainData['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3cdHnr_r2OqV",
    "outputId": "310df44f-3187-42e6-d534-aafb49f2ae4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "      <td>info found page mb pdf file wait untill team l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "      <td>team member drewes van der laag urllink mail r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "      <td>testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "      <td>thanks yahoo toolbar capture url popups mean s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  ...                                         clean_text\n",
       "0   male  ...  info found page mb pdf file wait untill team l...\n",
       "1   male  ...  team member drewes van der laag urllink mail r...\n",
       "2   male  ...  het kader van kernfusie op aarde maak je eigen...\n",
       "3   male  ...                                    testing testing\n",
       "4   male  ...  thanks yahoo toolbar capture url popups mean s...\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData['clean_text'] = trainData['text'].apply(lambda x: clean_data(x))\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "QIv7srD5sueq"
   },
   "outputs": [],
   "source": [
    "trainData['labels'] = trainData.apply(lambda x : [x['gender'],x['age'],x['topic'],x['sign']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rkYeOfYfh1c0"
   },
   "outputs": [],
   "source": [
    "#drop  gender,age,topic & sign as we merged to labels column\n",
    "trainData.drop(columns=['gender','age','topic','sign','text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Y1Zy7uHNzl4j",
    "outputId": "e73ae3b6-907b-42df-d388-2508ef26c276"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found page mb pdf file wait untill team l...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team member drewes van der laag urllink mail r...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture url popups mean s...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text                                   labels\n",
       "0  info found page mb pdf file wait untill team l...                 [male, 15, Student, Leo]\n",
       "1  team member drewes van der laag urllink mail r...                 [male, 15, Student, Leo]\n",
       "2  het kader van kernfusie op aarde maak je eigen...                 [male, 15, Student, Leo]\n",
       "3                                    testing testing                 [male, 15, Student, Leo]\n",
       "4  thanks yahoo toolbar capture url popups mean s...  [male, 33, InvestmentBanking, Aquarius]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w-EVXZhxjB4p"
   },
   "outputs": [],
   "source": [
    "#splitting X and Y\n",
    "X = trainData['clean_text']\n",
    "y = trainData['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUy0_40W0vvg"
   },
   "source": [
    "**Using CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vCJcTRMgq2Us",
    "outputId": "64d0df74-e3e5-43e1-c04f-701ad58de3bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 122017)\t1\n",
      "  (0, 86483)\t1\n",
      "  (0, 183680)\t1\n",
      "  (0, 156128)\t1\n",
      "  (0, 186938)\t1\n",
      "  (0, 81250)\t1\n",
      "  (0, 276549)\t1\n",
      "  (0, 271270)\t1\n",
      "  (0, 251004)\t1\n",
      "  (0, 135298)\t1\n",
      "  (0, 199300)\t1\n",
      "  (0, 135741)\t1\n",
      "  (0, 118515)\t1\n",
      "  (0, 86691)\t1\n",
      "  (0, 186939)\t1\n",
      "  (0, 276752)\t1\n",
      "  (0, 251072)\t1\n",
      "  (1, 251004)\t1\n",
      "  (1, 157561)\t1\n",
      "  (1, 274856)\t1\n",
      "  (1, 57956)\t1\n",
      "  (1, 271841)\t3\n",
      "  (1, 151141)\t3\n",
      "  (1, 294694)\t1\n",
      "  (1, 30550)\t1\n",
      "  :\t:\n",
      "  (99999, 187297)\t1\n",
      "  (99999, 155626)\t1\n",
      "  (99999, 30152)\t2\n",
      "  (99999, 266947)\t1\n",
      "  (99999, 108754)\t1\n",
      "  (99999, 225446)\t1\n",
      "  (99999, 244468)\t1\n",
      "  (99999, 10445)\t1\n",
      "  (99999, 253657)\t1\n",
      "  (99999, 240856)\t1\n",
      "  (99999, 74034)\t1\n",
      "  (99999, 10588)\t1\n",
      "  (99999, 125430)\t1\n",
      "  (99999, 225611)\t1\n",
      "  (99999, 188329)\t1\n",
      "  (99999, 227385)\t1\n",
      "  (99999, 141560)\t1\n",
      "  (99999, 213318)\t1\n",
      "  (99999, 250514)\t1\n",
      "  (99999, 31671)\t1\n",
      "  (99999, 253658)\t1\n",
      "  (99999, 244478)\t1\n",
      "  (99999, 198335)\t3\n",
      "  (99999, 100222)\t1\n",
      "  (99999, 31911)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2),max_df=0.9,min_df=5)\n",
    "X_cnt = vectorizer.fit_transform(X)\n",
    "print(X_cnt)  # returns the row number and column number of cells which have 1 as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FxMIgQpRv1pv",
    "outputId": "94e381ef-096d-4add-fb27-ab7bf9f7db46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x298244 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cnt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n0D3wGMGsn_U",
    "outputId": "f6204436-3e83-45a2-c06d-014d658dd879"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa meeting',\n",
       " 'aaa',\n",
       " 'aaaaaah',\n",
       " 'aaaaah',\n",
       " 'aaaaahhhhh',\n",
       " 'aaaah',\n",
       " 'aaaahhh',\n",
       " 'aaaand',\n",
       " 'aaah']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOlwItipKkNU"
   },
   "outputs": [],
   "source": [
    "label_counts=dict()\n",
    "\n",
    "for labels in trainData.labels.values:\n",
    "     for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[str(label)]+=1\n",
    "        else:\n",
    "            label_counts[str(label)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5g89GCNf2YFm",
    "outputId": "511e3531-33f9-44fd-b9ea-0e7621f5cf17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13': 1,\n",
       " '14': 1,\n",
       " '15': 1,\n",
       " '16': 1,\n",
       " '17': 1,\n",
       " '23': 1,\n",
       " '24': 1,\n",
       " '25': 1,\n",
       " '26': 1,\n",
       " '27': 1,\n",
       " '33': 1,\n",
       " '34': 1,\n",
       " '35': 1,\n",
       " '36': 1,\n",
       " '37': 1,\n",
       " '38': 1,\n",
       " '39': 1,\n",
       " '40': 1,\n",
       " '41': 1,\n",
       " '42': 1,\n",
       " '43': 1,\n",
       " '44': 1,\n",
       " '45': 1,\n",
       " '46': 1,\n",
       " '47': 1,\n",
       " '48': 1,\n",
       " 'Accounting': 409,\n",
       " 'Advertising': 778,\n",
       " 'Agriculture': 168,\n",
       " 'Aquarius': 9118,\n",
       " 'Architecture': 80,\n",
       " 'Aries': 10609,\n",
       " 'Arts': 5029,\n",
       " 'Automotive': 124,\n",
       " 'Banking': 344,\n",
       " 'Biotech': 324,\n",
       " 'BusinessServices': 620,\n",
       " 'Cancer': 9147,\n",
       " 'Capricorn': 8714,\n",
       " 'Chemicals': 305,\n",
       " 'Communications-Media': 2811,\n",
       " 'Construction': 250,\n",
       " 'Consulting': 937,\n",
       " 'Education': 5545,\n",
       " 'Engineering': 2317,\n",
       " 'Environment': 6,\n",
       " 'Fashion': 1898,\n",
       " 'Gemini': 8921,\n",
       " 'Government': 2054,\n",
       " 'HumanResources': 209,\n",
       " 'Internet': 2231,\n",
       " 'InvestmentBanking': 244,\n",
       " 'Law': 360,\n",
       " 'LawEnforcement-Security': 367,\n",
       " 'Leo': 8348,\n",
       " 'Libra': 7290,\n",
       " 'Manufacturing': 542,\n",
       " 'Maritime': 59,\n",
       " 'Marketing': 725,\n",
       " 'Military': 790,\n",
       " 'Museums-Libraries': 308,\n",
       " 'Non-Profit': 1377,\n",
       " 'Pisces': 7452,\n",
       " 'Publishing': 1071,\n",
       " 'RealEstate': 149,\n",
       " 'Religion': 1093,\n",
       " 'Sagittarius': 7336,\n",
       " 'Science': 1086,\n",
       " 'Scorpio': 7072,\n",
       " 'Sports-Recreation': 406,\n",
       " 'Student': 22614,\n",
       " 'Taurus': 8541,\n",
       " 'Technology': 8442,\n",
       " 'Telecommunications': 162,\n",
       " 'Tourism': 253,\n",
       " 'Transportation': 610,\n",
       " 'Virgo': 7452,\n",
       " 'female': 46528,\n",
       " 'indUnk': 32903,\n",
       " 'male': 53472}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwMyGlwul_hd",
    "outputId": "304d7ac1-0368-4f1b-966a-6f16d8866460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        [male, 15, Student, Leo]\n",
       "1                        [male, 15, Student, Leo]\n",
       "2                        [male, 15, Student, Leo]\n",
       "3                        [male, 15, Student, Leo]\n",
       "4         [male, 33, InvestmentBanking, Aquarius]\n",
       "                           ...                   \n",
       "100832                   [male, 17, Student, Leo]\n",
       "100833                   [male, 17, Student, Leo]\n",
       "100834                   [male, 17, Student, Leo]\n",
       "100835                   [male, 17, Student, Leo]\n",
       "100836                   [male, 17, Student, Leo]\n",
       "Name: labels, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Y5l9obnxK1_",
    "outputId": "85fd77ab-9b05-4754-9158-d9894b86f892"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:987: UserWarning: unknown class(es) [13, 14, 15, 16, 17, 23, 24, 25, 26, 27, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48] will be ignored\n",
      "  .format(sorted(unknown, key=str)))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "mlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n",
    "y_mlb = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d-Owq_wK3__S",
    "outputId": "3fe9b368-4722-4a9f-d45b-8c4a7ab73d43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_mlb[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8jjoguhm1wX",
    "outputId": "9836aa60-b96c-4514-dcb0-0f6c7874d717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (80000, 298244)\n",
      "y_train shape:  (80000, 80)\n",
      "x_test shape:  (20000, 298244)\n",
      "y_test shape:  (20000, 80)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cnt,y_mlb, random_state=2,test_size = 0.2)\n",
    "\n",
    "print('x_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqB5ZEQaq1ZV",
    "outputId": "54a7e383-59c9-48ea-ed1c-a5aead6e2f49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x298244 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 77 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRatc4MhrWbE",
    "outputId": "49e3098a-1ec4-4ac1-91b1-e18e094868ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEzc_9nt1NTm"
   },
   "source": [
    "**Using LogisticRegression with CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4BPrN9t4y_w",
    "outputId": "1516afb2-aa45-4592-c4ad-9de9cc964006"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='ovr', n_jobs=None,\n",
       "                                                 penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings # to ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "model = OneVsRestClassifier(lr)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaI694JE5pk5",
    "outputId": "11586888-67bc-472d-f9a6-d9a6490df01c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHs8v3pntjJl",
    "outputId": "e08ebd92-da3c-433a-9ed1-fab5b124d709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXqjVARQ1Q0_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_eval_scores():\n",
    "   ## evaluating model performance via accuracy score\n",
    "   print(\" accuracy score : \", accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "   ## evaluating performance of the model using f1 score \n",
    "   print(\" f1 score : \" ,f1_score(y_test, y_pred_test,average='micro'))\n",
    "\n",
    "   ## evaluating performance of the model using recall_score\n",
    "   print(\" recall_score : \", recall_score(y_test,y_pred_test,average='micro'))\n",
    "\n",
    "   ## evaluating performance of the model using precision_score\n",
    "   print(\" precision_score : \", precision_score(y_test,y_pred_test,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBQDRsqKrJsb",
    "outputId": "6850165d-afef-471b-8979-347ea0ab4ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score :  0.13175\n",
      " f1 score :  0.5015437167443999\n",
      " recall_score :  0.38175\n",
      " precision_score :  0.7309017805858702\n"
     ]
    }
   ],
   "source": [
    "print_eval_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81vaZezDu_jN"
   },
   "source": [
    "Got very less accuracy score i.e. 13% even though precision(percent of predictions) were correct is 73% But F1 score(correct positive predictions) is 50% only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMKKZCz0wbZ1",
    "outputId": "6bf17a2a-c252-424e-9393-0aa735b417a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.58      0.09      0.16        78\n",
      "          27       0.77      0.28      0.41       159\n",
      "          28       0.60      0.08      0.14        39\n",
      "          29       0.73      0.26      0.38      1810\n",
      "          30       0.00      0.00      0.00         9\n",
      "          31       0.69      0.23      0.35      2148\n",
      "          32       0.47      0.15      0.23       985\n",
      "          33       0.50      0.19      0.28        21\n",
      "          34       0.69      0.15      0.24        61\n",
      "          35       0.46      0.10      0.16        60\n",
      "          36       0.50      0.05      0.10       111\n",
      "          37       0.66      0.19      0.29      1878\n",
      "          38       0.77      0.22      0.34      1799\n",
      "          39       0.81      0.21      0.34        80\n",
      "          40       0.54      0.09      0.15       558\n",
      "          41       0.14      0.02      0.03        52\n",
      "          42       0.79      0.13      0.23       203\n",
      "          43       0.77      0.29      0.42      1105\n",
      "          44       0.78      0.25      0.38       489\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.86      0.37      0.51       401\n",
      "          47       0.71      0.20      0.31      1759\n",
      "          48       0.76      0.23      0.35       391\n",
      "          49       0.38      0.07      0.11        45\n",
      "          50       0.51      0.08      0.13       456\n",
      "          51       0.78      0.23      0.35        62\n",
      "          52       0.31      0.07      0.12        69\n",
      "          53       0.67      0.20      0.31        89\n",
      "          54       0.60      0.16      0.25      1620\n",
      "          55       0.82      0.28      0.42      1414\n",
      "          56       0.68      0.17      0.27       111\n",
      "          57       1.00      0.22      0.36         9\n",
      "          58       0.59      0.09      0.15       147\n",
      "          59       0.71      0.16      0.26       160\n",
      "          60       0.90      0.43      0.58        61\n",
      "          61       0.61      0.06      0.11       268\n",
      "          62       0.71      0.21      0.32      1435\n",
      "          63       0.81      0.25      0.38       224\n",
      "          64       0.20      0.03      0.06        31\n",
      "          65       0.59      0.17      0.27       186\n",
      "          66       0.53      0.15      0.24      1499\n",
      "          67       0.53      0.11      0.18       228\n",
      "          68       0.68      0.14      0.23      1424\n",
      "          69       0.74      0.17      0.28        99\n",
      "          70       0.68      0.39      0.50      4400\n",
      "          71       0.60      0.20      0.30      1689\n",
      "          72       0.72      0.22      0.33      1672\n",
      "          73       0.00      0.00      0.00        27\n",
      "          74       0.12      0.05      0.07        61\n",
      "          75       0.98      0.41      0.58       128\n",
      "          76       0.64      0.16      0.25      1525\n",
      "          77       0.78      0.60      0.68      9177\n",
      "          78       0.64      0.37      0.47      6664\n",
      "          79       0.79      0.69      0.74     10823\n",
      "\n",
      "   micro avg       0.73      0.38      0.50     60000\n",
      "   macro avg       0.41      0.13      0.19     60000\n",
      "weighted avg       0.71      0.38      0.47     60000\n",
      " samples avg       0.62      0.38      0.45     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f7jpfB8uuw7",
    "outputId": "0c0fa66d-1546-4140-a33b-ebe12d533cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuals\n",
      "[('Aries', 'Fashion', 'male'), ('Cancer', 'Student', 'male'), ('Sagittarius', 'indUnk', 'male'), ('Banking', 'Sagittarius', 'male'), ('Aries', 'Student', 'male')]\n",
      "\n",
      "predicted\n",
      "[('male',), ('Gemini', 'Student', 'female'), ('male',), ('male',), ('Student', 'male')]\n"
     ]
    }
   ],
   "source": [
    "print(\"actuals\")\n",
    "print(mlb.inverse_transform(y_test)[0:5])\n",
    "print(\"\")\n",
    "print(\"predicted\")\n",
    "print(mlb.inverse_transform(y_pred_test)[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_1EUmcpSfvv"
   },
   "source": [
    "working with **TFIDF** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i18ZDz6kVIJ4"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2),max_df=0.9,min_df=5 )\n",
    "tfidf_vectorizer.fit(X)\n",
    "X_tf = tfidf_vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7SWm7hMXArS",
    "outputId": "a7dc167a-688d-4706-9bdb-8985a5f30222"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x298244 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEwbN28eXH8H",
    "outputId": "e4cbb986-1948-41e2-f559-4d4cb62478f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa meeting',\n",
       " 'aaa',\n",
       " 'aaaaaah',\n",
       " 'aaaaah',\n",
       " 'aaaaahhhhh',\n",
       " 'aaaah',\n",
       " 'aaaahhh',\n",
       " 'aaaand',\n",
       " 'aaah']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_z-izCwPXLIK",
    "outputId": "5be72920-5442-45a8-9853-d8665ab3808a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'13': 1,\n",
       " '14': 1,\n",
       " '15': 1,\n",
       " '16': 1,\n",
       " '17': 1,\n",
       " '23': 1,\n",
       " '24': 1,\n",
       " '25': 1,\n",
       " '26': 1,\n",
       " '27': 1,\n",
       " '33': 1,\n",
       " '34': 1,\n",
       " '35': 1,\n",
       " '36': 1,\n",
       " '37': 1,\n",
       " '38': 1,\n",
       " '39': 1,\n",
       " '40': 1,\n",
       " '41': 1,\n",
       " '42': 1,\n",
       " '43': 1,\n",
       " '44': 1,\n",
       " '45': 1,\n",
       " '46': 1,\n",
       " '47': 1,\n",
       " '48': 1,\n",
       " 'Accounting': 409,\n",
       " 'Advertising': 778,\n",
       " 'Agriculture': 168,\n",
       " 'Aquarius': 9118,\n",
       " 'Architecture': 80,\n",
       " 'Aries': 10609,\n",
       " 'Arts': 5029,\n",
       " 'Automotive': 124,\n",
       " 'Banking': 344,\n",
       " 'Biotech': 324,\n",
       " 'BusinessServices': 620,\n",
       " 'Cancer': 9147,\n",
       " 'Capricorn': 8714,\n",
       " 'Chemicals': 305,\n",
       " 'Communications-Media': 2811,\n",
       " 'Construction': 250,\n",
       " 'Consulting': 937,\n",
       " 'Education': 5545,\n",
       " 'Engineering': 2317,\n",
       " 'Environment': 6,\n",
       " 'Fashion': 1898,\n",
       " 'Gemini': 8921,\n",
       " 'Government': 2054,\n",
       " 'HumanResources': 209,\n",
       " 'Internet': 2231,\n",
       " 'InvestmentBanking': 244,\n",
       " 'Law': 360,\n",
       " 'LawEnforcement-Security': 367,\n",
       " 'Leo': 8348,\n",
       " 'Libra': 7290,\n",
       " 'Manufacturing': 542,\n",
       " 'Maritime': 59,\n",
       " 'Marketing': 725,\n",
       " 'Military': 790,\n",
       " 'Museums-Libraries': 308,\n",
       " 'Non-Profit': 1377,\n",
       " 'Pisces': 7452,\n",
       " 'Publishing': 1071,\n",
       " 'RealEstate': 149,\n",
       " 'Religion': 1093,\n",
       " 'Sagittarius': 7336,\n",
       " 'Science': 1086,\n",
       " 'Scorpio': 7072,\n",
       " 'Sports-Recreation': 406,\n",
       " 'Student': 22614,\n",
       " 'Taurus': 8541,\n",
       " 'Technology': 8442,\n",
       " 'Telecommunications': 162,\n",
       " 'Tourism': 253,\n",
       " 'Transportation': 610,\n",
       " 'Virgo': 7452,\n",
       " 'female': 46528,\n",
       " 'indUnk': 32903,\n",
       " 'male': 53472}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts=dict()\n",
    "\n",
    "for labels in trainData.labels.values:\n",
    "     for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[str(label)]+=1\n",
    "        else:\n",
    "            label_counts[str(label)]=1\n",
    "\n",
    "\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3zthG7nXUx0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "mlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n",
    "y_mlb = mlb.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bp-JM9-T1_pJ",
    "outputId": "65e482a2-3ff7-4153-fd8c-a09a64508cc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['13', '14', '15', '16', '17', '23', '24', '25', '26', '27', '33',\n",
       "       '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44',\n",
       "       '45', '46', '47', '48', 'Accounting', 'Advertising', 'Agriculture',\n",
       "       'Aquarius', 'Architecture', 'Aries', 'Arts', 'Automotive',\n",
       "       'Banking', 'Biotech', 'BusinessServices', 'Cancer', 'Capricorn',\n",
       "       'Chemicals', 'Communications-Media', 'Construction', 'Consulting',\n",
       "       'Education', 'Engineering', 'Environment', 'Fashion', 'Gemini',\n",
       "       'Government', 'HumanResources', 'Internet', 'InvestmentBanking',\n",
       "       'Law', 'LawEnforcement-Security', 'Leo', 'Libra', 'Manufacturing',\n",
       "       'Maritime', 'Marketing', 'Military', 'Museums-Libraries',\n",
       "       'Non-Profit', 'Pisces', 'Publishing', 'RealEstate', 'Religion',\n",
       "       'Sagittarius', 'Science', 'Scorpio', 'Sports-Recreation',\n",
       "       'Student', 'Taurus', 'Technology', 'Telecommunications', 'Tourism',\n",
       "       'Transportation', 'Virgo', 'female', 'indUnk', 'male'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCbCGFAXXZl5",
    "outputId": "a89997fc-7af3-4e69-a436-fbcc7e9ff048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (80000, 298244)\n",
      "y_train shape:  (80000, 80)\n",
      "x_test shape:  (20000, 298244)\n",
      "y_test shape:  (20000, 80)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tf,y_mlb, random_state=2,test_size = 0.2)\n",
    "\n",
    "print('x_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('x_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yY16-eYdzncW"
   },
   "source": [
    "**Using Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMqzORPyYRcb",
    "outputId": "e18e8f1d-9687-4530-f531-b8e6b3d4986f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import warnings # to ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "model1 = OneVsRestClassifier(nb)\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred_test = model1.predict(X_test)\n",
    "print(y_pred_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gNwLU4AsZpWk",
    "outputId": "d9dc423d-e0cc-4418-92c1-c4fa9facbf72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00        78\n",
      "          27       1.00      0.06      0.11       159\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       1.00      0.01      0.01      1810\n",
      "          30       0.00      0.00      0.00         9\n",
      "          31       1.00      0.02      0.05      2148\n",
      "          32       0.82      0.01      0.03       985\n",
      "          33       0.00      0.00      0.00        21\n",
      "          34       0.00      0.00      0.00        61\n",
      "          35       0.00      0.00      0.00        60\n",
      "          36       0.00      0.00      0.00       111\n",
      "          37       0.90      0.02      0.05      1878\n",
      "          38       1.00      0.00      0.00      1799\n",
      "          39       0.00      0.00      0.00        80\n",
      "          40       0.00      0.00      0.00       558\n",
      "          41       0.00      0.00      0.00        52\n",
      "          42       0.00      0.00      0.00       203\n",
      "          43       1.00      0.01      0.02      1105\n",
      "          44       0.00      0.00      0.00       489\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.00      0.00      0.00       401\n",
      "          47       1.00      0.02      0.04      1759\n",
      "          48       0.00      0.00      0.00       391\n",
      "          49       0.00      0.00      0.00        45\n",
      "          50       0.00      0.00      0.00       456\n",
      "          51       0.00      0.00      0.00        62\n",
      "          52       0.00      0.00      0.00        69\n",
      "          53       0.00      0.00      0.00        89\n",
      "          54       1.00      0.00      0.00      1620\n",
      "          55       1.00      0.01      0.03      1414\n",
      "          56       0.00      0.00      0.00       111\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.00      0.00      0.00       147\n",
      "          59       0.00      0.00      0.00       160\n",
      "          60       0.00      0.00      0.00        61\n",
      "          61       0.00      0.00      0.00       268\n",
      "          62       1.00      0.04      0.08      1435\n",
      "          63       0.00      0.00      0.00       224\n",
      "          64       0.00      0.00      0.00        31\n",
      "          65       0.00      0.00      0.00       186\n",
      "          66       0.40      0.00      0.00      1499\n",
      "          67       0.00      0.00      0.00       228\n",
      "          68       1.00      0.00      0.01      1424\n",
      "          69       0.00      0.00      0.00        99\n",
      "          70       0.89      0.05      0.09      4400\n",
      "          71       1.00      0.00      0.01      1689\n",
      "          72       1.00      0.01      0.01      1672\n",
      "          73       0.00      0.00      0.00        27\n",
      "          74       0.00      0.00      0.00        61\n",
      "          75       1.00      0.25      0.40       128\n",
      "          76       1.00      0.00      0.00      1525\n",
      "          77       0.71      0.70      0.71      9177\n",
      "          78       0.93      0.04      0.08      6664\n",
      "          79       0.75      0.76      0.76     10823\n",
      "\n",
      "   micro avg       0.74      0.26      0.38     60000\n",
      "   macro avg       0.24      0.03      0.03     60000\n",
      "weighted avg       0.79      0.26      0.27     60000\n",
      " samples avg       0.73      0.26      0.38     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80dTDNKhZuX0",
    "outputId": "83fb1cc1-f4f8-4c5b-ae57-6d2d28139c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score :  0.0111\n",
      " f1 score :  0.38370899739116454\n",
      " recall_score :  0.25861666666666666\n",
      " precision_score :  0.7431869342401456\n"
     ]
    }
   ],
   "source": [
    "print_eval_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRsQvzckv7lw"
   },
   "source": [
    "Naive Bayes performed very poor. Got very less accuracy 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDduETnc7Yqi",
    "outputId": "2904497a-6a73-4c4c-8bed-a157608ffdeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuals : ('Capricorn', 'Student', 'female')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Advertising', 'Cancer', 'male')\n",
      "predicted : ('Advertising', 'Cancer', 'male')\n",
      "\n",
      "actuals : ('Cancer', 'female', 'indUnk')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Pisces', 'female', 'indUnk')\n",
      "predicted : ('female',)\n",
      "\n",
      "actuals : ('Libra', 'Publishing', 'male')\n",
      "predicted : ('male',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_list = random.sample(range(len(y_test)), 5)\n",
    "\n",
    "for i in num_list:\n",
    "  print(\"actuals :\" , mlb.inverse_transform(y_test)[i])\n",
    "  print(\"predicted :\", mlb.inverse_transform(y_pred_test)[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pYEqqNUwPB4"
   },
   "source": [
    "We got one lucky prediction :D Identified all correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9ZvnJVqzxk5"
   },
   "source": [
    "***Using LogisticRegression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN4Tl2ZeXhJd",
    "outputId": "b692d8d9-5cfc-4aa4-bdf7-4d2527a3c8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import warnings # to ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='ovr')\n",
    "model = OneVsRestClassifier(lr)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "print(y_pred_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqHYO3vFoty6",
    "outputId": "1e78a810-429c-4b5a-935b-eefbb2116c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00        78\n",
      "          27       1.00      0.04      0.08       159\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       1.00      0.06      0.12      1810\n",
      "          30       0.00      0.00      0.00         9\n",
      "          31       0.99      0.07      0.12      2148\n",
      "          32       0.83      0.01      0.02       985\n",
      "          33       0.00      0.00      0.00        21\n",
      "          34       0.00      0.00      0.00        61\n",
      "          35       0.00      0.00      0.00        60\n",
      "          36       0.00      0.00      0.00       111\n",
      "          37       1.00      0.03      0.06      1878\n",
      "          38       1.00      0.02      0.05      1799\n",
      "          39       0.00      0.00      0.00        80\n",
      "          40       0.00      0.00      0.00       558\n",
      "          41       0.00      0.00      0.00        52\n",
      "          42       0.00      0.00      0.00       203\n",
      "          43       0.98      0.10      0.18      1105\n",
      "          44       1.00      0.03      0.06       489\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.97      0.09      0.16       401\n",
      "          47       1.00      0.04      0.07      1759\n",
      "          48       1.00      0.04      0.08       391\n",
      "          49       0.00      0.00      0.00        45\n",
      "          50       1.00      0.00      0.01       456\n",
      "          51       0.00      0.00      0.00        62\n",
      "          52       0.00      0.00      0.00        69\n",
      "          53       1.00      0.01      0.02        89\n",
      "          54       0.96      0.01      0.03      1620\n",
      "          55       1.00      0.05      0.09      1414\n",
      "          56       1.00      0.01      0.02       111\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.00      0.00      0.00       147\n",
      "          59       0.00      0.00      0.00       160\n",
      "          60       1.00      0.05      0.09        61\n",
      "          61       0.00      0.00      0.00       268\n",
      "          62       0.96      0.08      0.14      1435\n",
      "          63       0.50      0.01      0.03       224\n",
      "          64       0.00      0.00      0.00        31\n",
      "          65       1.00      0.01      0.01       186\n",
      "          66       0.91      0.02      0.04      1499\n",
      "          67       0.67      0.01      0.02       228\n",
      "          68       1.00      0.02      0.04      1424\n",
      "          69       0.00      0.00      0.00        99\n",
      "          70       0.87      0.13      0.23      4400\n",
      "          71       0.96      0.03      0.05      1689\n",
      "          72       0.94      0.05      0.09      1672\n",
      "          73       0.00      0.00      0.00        27\n",
      "          74       0.00      0.00      0.00        61\n",
      "          75       1.00      0.29      0.45       128\n",
      "          76       1.00      0.01      0.02      1525\n",
      "          77       0.84      0.48      0.61      9177\n",
      "          78       0.92      0.09      0.17      6664\n",
      "          79       0.83      0.59      0.69     10823\n",
      "\n",
      "   micro avg       0.85      0.22      0.34     60000\n",
      "   macro avg       0.36      0.03      0.05     60000\n",
      "weighted avg       0.87      0.22      0.29     60000\n",
      " samples avg       0.54      0.22      0.30     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aJftkNvow0x",
    "outputId": "76fc8b8e-0165-49a4-c69f-70901fc45691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score :  0.03135\n",
      " f1 score :  0.34489809466905663\n",
      " recall_score :  0.21646666666666667\n",
      " precision_score :  0.8480574600065296\n"
     ]
    }
   ],
   "source": [
    "print_eval_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9B4u5UYwc9Q"
   },
   "source": [
    "Accuracy has been increased with Logistic regression and TF-IDF.\n",
    "Percentage of postive cases we catch (Recall # 21.6%) is very low and correct positive preditions(F1 score# 34.5) are also very less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9eW50p4pLlz",
    "outputId": "2b8fa327-a026-40a1-f2a1-dd74cd3cdfc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuals : ('Libra', 'female', 'indUnk')\n",
      "predicted : ()\n",
      "\n",
      "actuals : ('Manufacturing', 'Virgo', 'male')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Taurus', 'female', 'indUnk')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Aquarius', 'Marketing', 'female')\n",
      "predicted : ('female',)\n",
      "\n",
      "actuals : ('Sagittarius', 'female', 'indUnk')\n",
      "predicted : ('female',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_list = random.sample(range(len(y_test)), 5)\n",
    "\n",
    "for i in num_list:\n",
    "  print(\"actuals :\" , mlb.inverse_transform(y_test)[i])\n",
    "  print(\"predicted :\", mlb.inverse_transform(y_pred_test)[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVgiliy3zb2X"
   },
   "source": [
    "**Using SVC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "M2HVGm1fsiJM",
    "outputId": "52df887f-e268-45b0-a8d6-bcbbd21b5485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LinearSVC(C=1.0, class_weight=None, dual=False,\n",
       "                                        fit_intercept=True, intercept_scaling=1,\n",
       "                                        loss='squared_hinge', max_iter=1000,\n",
       "                                        multi_class='ovr', penalty='l1',\n",
       "                                        random_state=None, tol=0.0001,\n",
       "                                        verbose=0),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(penalty='l1', dual=False, loss='squared_hinge')\n",
    "model2 = OneVsRestClassifier(svc)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ky-k9r3QtCc2",
    "outputId": "793e3499-be13-4801-a04c-9c35feac762e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model.predict(X_test)\n",
    "print(y_pred_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hHpD5lA-tDq_",
    "outputId": "e9e59780-7a91-438d-a5da-8e1a4163bd3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           9       0.00      0.00      0.00         0\n",
      "          10       0.00      0.00      0.00         0\n",
      "          11       0.00      0.00      0.00         0\n",
      "          12       0.00      0.00      0.00         0\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.00      0.00      0.00         0\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         0\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         0\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00        78\n",
      "          27       1.00      0.04      0.08       159\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       1.00      0.06      0.12      1810\n",
      "          30       0.00      0.00      0.00         9\n",
      "          31       0.99      0.07      0.12      2148\n",
      "          32       0.83      0.01      0.02       985\n",
      "          33       0.00      0.00      0.00        21\n",
      "          34       0.00      0.00      0.00        61\n",
      "          35       0.00      0.00      0.00        60\n",
      "          36       0.00      0.00      0.00       111\n",
      "          37       1.00      0.03      0.06      1878\n",
      "          38       1.00      0.02      0.05      1799\n",
      "          39       0.00      0.00      0.00        80\n",
      "          40       0.00      0.00      0.00       558\n",
      "          41       0.00      0.00      0.00        52\n",
      "          42       0.00      0.00      0.00       203\n",
      "          43       0.98      0.10      0.18      1105\n",
      "          44       1.00      0.03      0.06       489\n",
      "          45       0.00      0.00      0.00         1\n",
      "          46       0.97      0.09      0.16       401\n",
      "          47       1.00      0.04      0.07      1759\n",
      "          48       1.00      0.04      0.08       391\n",
      "          49       0.00      0.00      0.00        45\n",
      "          50       1.00      0.00      0.01       456\n",
      "          51       0.00      0.00      0.00        62\n",
      "          52       0.00      0.00      0.00        69\n",
      "          53       1.00      0.01      0.02        89\n",
      "          54       0.96      0.01      0.03      1620\n",
      "          55       1.00      0.05      0.09      1414\n",
      "          56       1.00      0.01      0.02       111\n",
      "          57       0.00      0.00      0.00         9\n",
      "          58       0.00      0.00      0.00       147\n",
      "          59       0.00      0.00      0.00       160\n",
      "          60       1.00      0.05      0.09        61\n",
      "          61       0.00      0.00      0.00       268\n",
      "          62       0.96      0.08      0.14      1435\n",
      "          63       0.50      0.01      0.03       224\n",
      "          64       0.00      0.00      0.00        31\n",
      "          65       1.00      0.01      0.01       186\n",
      "          66       0.91      0.02      0.04      1499\n",
      "          67       0.67      0.01      0.02       228\n",
      "          68       1.00      0.02      0.04      1424\n",
      "          69       0.00      0.00      0.00        99\n",
      "          70       0.87      0.13      0.23      4400\n",
      "          71       0.96      0.03      0.05      1689\n",
      "          72       0.94      0.05      0.09      1672\n",
      "          73       0.00      0.00      0.00        27\n",
      "          74       0.00      0.00      0.00        61\n",
      "          75       1.00      0.29      0.45       128\n",
      "          76       1.00      0.01      0.02      1525\n",
      "          77       0.84      0.48      0.61      9177\n",
      "          78       0.92      0.09      0.17      6664\n",
      "          79       0.83      0.59      0.69     10823\n",
      "\n",
      "   micro avg       0.85      0.22      0.34     60000\n",
      "   macro avg       0.36      0.03      0.05     60000\n",
      "weighted avg       0.87      0.22      0.29     60000\n",
      " samples avg       0.54      0.22      0.30     60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU4JbsIP8uCU"
   },
   "source": [
    "average=micro says the function to compute f1 by considering total true positives, false negatives and false positives (no matter of the prediction for each label in the dataset)\n",
    "\n",
    "average=macro says the function to compute f1 for each label, and returns the average without considering the proportion for each label in the dataset.\n",
    "\n",
    "average=weighted says the function to compute f1 for each label, and returns the average considering the proportion for each label in the dataset.\n",
    "\n",
    "average=samples says the function to compute f1 for each instance, and returns the average. Use it for multilabel classification.\n",
    "\n",
    "Here are the evaluation scores for Micro average\n",
    "f1 score :  0.34\n",
    "recall_score :  0.21\n",
    "precision_score :  0.84\n",
    "\n",
    "Precision – What percent of your predictions were correct \n",
    "Recall – What percent of the positive cases did you catch \n",
    "F1 score – What percent of positive predictions were correct \n",
    "support is the number of occurence of the given class in your dataset\n",
    "\n",
    "For class 79, We have 83% predictions(Precision) correct and able to caught 59%(Recall) positive cases correctly and 69%(F1 Score) of postive predictions are correct\n",
    "\n",
    "Support numbers- For every 60000 occurences we have 10823 occurances for class 79.\n",
    "\n",
    "Overall accuracy is 31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MRP7m3jlFjpa",
    "outputId": "6d4e5705-9b98-4b4d-e442-6e55989f0539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy score :  0.03135\n",
      " f1 score :  0.34489809466905663\n",
      " recall_score :  0.21646666666666667\n",
      " precision_score :  0.8480574600065296\n"
     ]
    }
   ],
   "source": [
    "#Micro average Evaluation \n",
    "print_eval_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRD6GtusxSrm"
   },
   "source": [
    "SVC and LR - both worked very similar. Got similar evaluaiton scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HYUtWq9XqDS6",
    "outputId": "3e32638a-b295-430f-def1-ed8b6b84bef9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actuals : ('Capricorn', 'female', 'indUnk')\n",
      "predicted : ('female',)\n",
      "\n",
      "actuals : ('Gemini', 'indUnk', 'male')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Capricorn', 'Engineering', 'male')\n",
      "predicted : ('male',)\n",
      "\n",
      "actuals : ('Sagittarius', 'female', 'indUnk')\n",
      "predicted : ('female', 'indUnk')\n",
      "\n",
      "actuals : ('Aquarius', 'female', 'indUnk')\n",
      "predicted : ('female',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_list = random.sample(range(len(y_test)), 5)\n",
    "\n",
    "for i in num_list:\n",
    "  print(\"actuals :\" , mlb.inverse_transform(y_test)[i])\n",
    "  print(\"predicted :\", mlb.inverse_transform(y_pred_test)[i])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DpqNtDJISvr"
   },
   "source": [
    "# **PART 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LD0_rg8yIWxn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/AIML/LABS/NLP/Project2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cYTrJxzNcp6p"
   },
   "outputs": [],
   "source": [
    "data_file = open('GL Bot.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QwBAMN_rd9R7",
    "outputId": "4d7546de-93e5-4dc9-8511-539d7b01b47a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'context_set': '',\n",
       "   'patterns': ['hi',\n",
       "    'how are you',\n",
       "    'is anyone there',\n",
       "    'hello',\n",
       "    'whats up',\n",
       "    'hey',\n",
       "    'yo',\n",
       "    'listen',\n",
       "    'please help me',\n",
       "    'i am learner from',\n",
       "    'i belong to',\n",
       "    'aiml batch',\n",
       "    'aifl batch',\n",
       "    'i am from',\n",
       "    'my pm is',\n",
       "    'blended',\n",
       "    'online',\n",
       "    'i am from',\n",
       "    'hey ya',\n",
       "    'talking to you for first time'],\n",
       "   'responses': ['Hello! how can i help you ?'],\n",
       "   'tag': 'Intro'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['thank you',\n",
       "    'thanks',\n",
       "    'cya',\n",
       "    'see you',\n",
       "    'later',\n",
       "    'see you later',\n",
       "    'goodbye',\n",
       "    'i am leaving',\n",
       "    'have a Good day',\n",
       "    'you helped me',\n",
       "    'thanks a lot',\n",
       "    'thanks a ton',\n",
       "    'you are the best',\n",
       "    'great help',\n",
       "    'too good',\n",
       "    'you are a good learning buddy'],\n",
       "   'responses': ['I hope I was able to assist you, Good Bye'],\n",
       "   'tag': 'Exit'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['olympus',\n",
       "    'explain me how olympus works',\n",
       "    'I am not able to understand olympus',\n",
       "    'olympus window not working',\n",
       "    'no access to olympus',\n",
       "    'unable to see link in olympus',\n",
       "    'no link visible on olympus',\n",
       "    'whom to contact for olympus',\n",
       "    'lot of problem with olympus',\n",
       "    'olypus is not a good tool',\n",
       "    'lot of problems with olympus',\n",
       "    'how to use olympus',\n",
       "    'teach me olympus'],\n",
       "   'responses': ['Link: Olympus wiki'],\n",
       "   'tag': 'Olympus'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['i am not able to understand svm',\n",
       "    'explain me how machine learning works',\n",
       "    'i am not able to understand naive bayes',\n",
       "    'i am not able to understand logistic regression',\n",
       "    'i am not able to understand ensemble techb=niques',\n",
       "    'i am not able to understand knn',\n",
       "    'i am not able to understand knn imputer',\n",
       "    'i am not able to understand cross validation',\n",
       "    'i am not able to understand boosting',\n",
       "    'i am not able to understand random forest',\n",
       "    'i am not able to understand ada boosting',\n",
       "    'i am not able to understand gradient boosting',\n",
       "    'machine learning',\n",
       "    'ML',\n",
       "    'SL',\n",
       "    'supervised learning',\n",
       "    'knn',\n",
       "    'logistic regression',\n",
       "    'regression',\n",
       "    'classification',\n",
       "    'naive bayes',\n",
       "    'nb',\n",
       "    'ensemble techniques',\n",
       "    'bagging',\n",
       "    'boosting',\n",
       "    'ada boosting',\n",
       "    'ada',\n",
       "    'gradient boosting',\n",
       "    'hyper parameters'],\n",
       "   'responses': ['Link: Machine Learning wiki '],\n",
       "   'tag': 'SL'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what is deep learning',\n",
       "    'unable to understand deep learning',\n",
       "    'explain me how deep learning works',\n",
       "    'i am not able to understand deep learning',\n",
       "    'not able to understand neural nets',\n",
       "    'very diffult to understand neural nets',\n",
       "    'unable to understand neural nets',\n",
       "    'ann',\n",
       "    'artificial intelligence',\n",
       "    'artificial neural networks',\n",
       "    'weights',\n",
       "    'activation function',\n",
       "    'hidden layers',\n",
       "    'softmax',\n",
       "    'sigmoid',\n",
       "    'relu',\n",
       "    'otimizer',\n",
       "    'forward propagation',\n",
       "    'backward propagation',\n",
       "    'epochs',\n",
       "    'epoch',\n",
       "    'what is an epoch',\n",
       "    'adam',\n",
       "    'sgd'],\n",
       "   'responses': ['Link: Neural Nets wiki'],\n",
       "   'tag': 'NN'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what is your name',\n",
       "    'who are you',\n",
       "    'name please',\n",
       "    'when are your hours of opertions',\n",
       "    'what are your working hours',\n",
       "    'hours of operation',\n",
       "    'working hours',\n",
       "    'hours'],\n",
       "   'responses': ['I am your virtual learning assistant'],\n",
       "   'tag': 'Bot'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['what the hell',\n",
       "    'bloody stupid bot',\n",
       "    'do you think you are very smart',\n",
       "    'screw you',\n",
       "    'i hate you',\n",
       "    'you are stupid',\n",
       "    'jerk',\n",
       "    'you are a joke',\n",
       "    'useless piece of shit'],\n",
       "   'responses': ['Please use respectful words'],\n",
       "   'tag': 'Profane'},\n",
       "  {'context_set': '',\n",
       "   'patterns': ['my problem is not solved',\n",
       "    'you did not help me',\n",
       "    'not a good solution',\n",
       "    'bad solution',\n",
       "    'not good solution',\n",
       "    'no help',\n",
       "    'wasted my time',\n",
       "    'useless bot',\n",
       "    'create a ticket'],\n",
       "   'responses': ['Tarnsferring the request to your PM'],\n",
       "   'tag': 'Ticket'}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9cfokgVcxTQ",
    "outputId": "7f8bab4b-215c-4e93-8b09-779281c956e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'how are you',\n",
       " 'is anyone there',\n",
       " 'hello',\n",
       " 'whats up',\n",
       " 'hey',\n",
       " 'yo',\n",
       " 'listen',\n",
       " 'please help me',\n",
       " 'i am learner from',\n",
       " 'i belong to',\n",
       " 'aiml batch',\n",
       " 'aifl batch',\n",
       " 'i am from',\n",
       " 'my pm is',\n",
       " 'blended',\n",
       " 'online',\n",
       " 'i am from',\n",
       " 'hey ya',\n",
       " 'talking to you for first time']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][0]['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "e-7ySTz3eC_9",
    "outputId": "e383dc0e-3ba6-4355-89c7-4df717c3ed52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Intro'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][0]['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8JUQozVeGlN",
    "outputId": "7ef99030-04f0-4a09-8d8a-f1792805be5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello! how can i help you ?']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][0]['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeWKB1FxkVqn",
    "outputId": "434cfad0-524e-456d-ba39-461f3a42f25c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is your name',\n",
       " 'who are you',\n",
       " 'name please',\n",
       " 'when are your hours of opertions',\n",
       " 'what are your working hours',\n",
       " 'hours of operation',\n",
       " 'working hours',\n",
       " 'hours']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][5]['patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "y0pZqB0KkcTb",
    "outputId": "0b15ae46-6d7c-40ad-bd6c-d13ae647d805"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Bot'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][5]['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBCBnkvnkie1",
    "outputId": "89aa2a5a-b04f-4f90-d1ac-bc291baaedd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am your virtual learning assistant']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents['intents'][5]['responses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VcKA9EjsmDQY"
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "documents = []\n",
    "classes = []\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kndRZSbtm2_p",
    "outputId": "1c772426-6541-4b98-8949-5dd23d2bfffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 documents\n",
      "8 classes ['Bot', 'Exit', 'Intro', 'NN', 'Olympus', 'Profane', 'SL', 'Ticket']\n",
      "158 unique lemmatized words ['a', 'able', 'access', 'activation', 'ada', 'adam', 'aifl', 'aiml', 'am', 'an', 'ann', 'anyone', 'are', 'artificial', 'backward', 'bad', 'bagging', 'batch', 'bayes', 'belong', 'best', 'blended', 'bloody', 'boosting', 'bot', 'buddy', 'classification', 'contact', 'create', 'cross', 'cya', 'day', 'deep', 'did', 'diffult', 'do', 'ensemble', 'epoch', 'explain', 'first', 'for', 'forest', 'forward', 'from', 'function', 'good', 'goodbye', 'gradient', 'great', 'hate', 'have', 'hell', 'hello', 'help', 'helped', 'hey', 'hi', 'hidden', 'hour', 'how', 'hyper', 'i', 'imputer', 'in', 'intelligence', 'is', 'jerk', 'joke', 'knn', 'later', 'layer', 'learner', 'learning', 'leaving', 'link', 'listen', 'logistic', 'lot', 'machine', 'me', 'ml', 'my', 'naive', 'name', 'nb', 'net', 'network', 'neural', 'no', 'not', 'of', 'olympus', 'olypus', 'on', 'online', 'operation', 'opertions', 'otimizer', 'parameter', 'piece', 'please', 'pm', 'problem', 'propagation', 'random', 'regression', 'relu', 'screw', 'see', 'sgd', 'shit', 'sigmoid', 'sl', 'smart', 'softmax', 'solution', 'solved', 'stupid', 'supervised', 'svm', 'talking', 'teach', 'techb=niques', 'technique', 'thank', 'thanks', 'the', 'there', 'think', 'ticket', 'time', 'to', 'ton', 'too', 'tool', 'unable', 'understand', 'up', 'use', 'useless', 'validation', 'very', 'visible', 'wasted', 'weight', 'what', 'whats', 'when', 'who', 'whom', 'window', 'with', 'work', 'working', 'ya', 'yo', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "words = [wordnet_lemmatizer.lemmatize(w.lower()) for w in words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9zsijM6sE86",
    "outputId": "90bb0bd8-eada-4fad-e443-da6589796650"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = []\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [wordnet_lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    train.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(train)\n",
    "train = np.array(train)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "X = np.array(list(train[:,0]))\n",
    "y = np.array(list(train[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfTlBV79Ikeb",
    "outputId": "3c4e4c46-3323-486d-c091-e27b7e24d78e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is  (128, 158)\n",
      "y shape is  (128, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape is \" , X.shape)\n",
    "print(\"y shape is \" , y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hx15Wt8UudVG",
    "outputId": "d4592c70-69ed-40eb-e740-c59eaf525f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "26/26 [==============================] - 1s 2ms/step - loss: 1.9995 - accuracy: 0.1406 \n",
      "Epoch 2/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.7269 - accuracy: 0.3594\n",
      "Epoch 3/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.3887 - accuracy: 0.5312\n",
      "Epoch 4/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.0638 - accuracy: 0.6016\n",
      "Epoch 5/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.7891\n",
      "Epoch 6/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.8516\n",
      "Epoch 7/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7969\n",
      "Epoch 8/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8750\n",
      "Epoch 9/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2920 - accuracy: 0.9062\n",
      "Epoch 10/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9219\n",
      "Epoch 11/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1157 - accuracy: 0.9766\n",
      "Epoch 12/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9531\n",
      "Epoch 13/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9453\n",
      "Epoch 14/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1853 - accuracy: 0.9375\n",
      "Epoch 15/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9531\n",
      "Epoch 16/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9141\n",
      "Epoch 17/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2026 - accuracy: 0.9531\n",
      "Epoch 18/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1500 - accuracy: 0.9297\n",
      "Epoch 19/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.9375\n",
      "Epoch 20/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9609\n",
      "Epoch 21/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9688\n",
      "Epoch 22/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9609\n",
      "Epoch 23/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9844\n",
      "Epoch 24/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9766\n",
      "Epoch 25/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9688\n",
      "Epoch 26/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9609\n",
      "Epoch 27/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9609\n",
      "Epoch 28/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9453\n",
      "Epoch 29/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1392 - accuracy: 0.9766\n",
      "Epoch 30/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0688 - accuracy: 0.9766\n",
      "Epoch 31/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1808 - accuracy: 0.9531\n",
      "Epoch 32/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1052 - accuracy: 0.9844\n",
      "Epoch 33/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9766\n",
      "Epoch 34/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.9531\n",
      "Epoch 35/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9609\n",
      "Epoch 36/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2284 - accuracy: 0.9453\n",
      "Epoch 37/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9609\n",
      "Epoch 38/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9609\n",
      "Epoch 39/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.9609\n",
      "Epoch 40/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2349 - accuracy: 0.9531\n",
      "Epoch 41/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2224 - accuracy: 0.9453\n",
      "Epoch 42/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0759 - accuracy: 0.9766\n",
      "Epoch 43/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1472 - accuracy: 0.9609\n",
      "Epoch 44/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9688\n",
      "Epoch 46/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1518 - accuracy: 0.9766\n",
      "Epoch 47/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9766\n",
      "Epoch 48/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.9844\n",
      "Epoch 49/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9844\n",
      "Epoch 50/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1300 - accuracy: 0.9453\n",
      "Epoch 51/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0552 - accuracy: 0.9922\n",
      "Epoch 52/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1330 - accuracy: 0.9844\n",
      "Epoch 53/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9844\n",
      "Epoch 54/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.9531\n",
      "Epoch 55/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9844\n",
      "Epoch 56/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2080 - accuracy: 0.9688\n",
      "Epoch 57/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9922\n",
      "Epoch 58/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2139 - accuracy: 0.9766\n",
      "Epoch 59/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2143 - accuracy: 0.9609\n",
      "Epoch 61/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9766\n",
      "Epoch 62/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1787 - accuracy: 0.9766\n",
      "Epoch 63/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0935 - accuracy: 0.9688\n",
      "Epoch 64/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.9609\n",
      "Epoch 65/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1393 - accuracy: 0.9922\n",
      "Epoch 66/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0325 - accuracy: 0.9922\n",
      "Epoch 67/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9844\n",
      "Epoch 68/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9688\n",
      "Epoch 69/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0926 - accuracy: 0.9766\n",
      "Epoch 70/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1796 - accuracy: 0.9609\n",
      "Epoch 71/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2091 - accuracy: 0.9688\n",
      "Epoch 72/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9766\n",
      "Epoch 73/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9844\n",
      "Epoch 74/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0834 - accuracy: 0.9688\n",
      "Epoch 75/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2274 - accuracy: 0.9688\n",
      "Epoch 76/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0846 - accuracy: 0.9766\n",
      "Epoch 77/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9766\n",
      "Epoch 78/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0811 - accuracy: 0.9766\n",
      "Epoch 80/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9766\n",
      "Epoch 81/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9688\n",
      "Epoch 82/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9922\n",
      "Epoch 83/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9766\n",
      "Epoch 84/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1416 - accuracy: 0.9844\n",
      "Epoch 85/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9844\n",
      "Epoch 86/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.9922\n",
      "Epoch 87/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9688\n",
      "Epoch 88/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9844\n",
      "Epoch 89/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9766\n",
      "Epoch 90/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9844\n",
      "Epoch 91/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9844\n",
      "Epoch 92/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0454 - accuracy: 0.9766\n",
      "Epoch 93/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9844\n",
      "Epoch 94/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.9453\n",
      "Epoch 95/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.9766\n",
      "Epoch 96/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9766\n",
      "Epoch 97/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.4150 - accuracy: 0.9766\n",
      "Epoch 98/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9453\n",
      "Epoch 99/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.9531\n",
      "Epoch 100/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2964 - accuracy: 0.9609\n",
      "Epoch 101/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.9766\n",
      "Epoch 102/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9844\n",
      "Epoch 103/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9922\n",
      "Epoch 104/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1401 - accuracy: 0.9766\n",
      "Epoch 105/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1187 - accuracy: 0.9766\n",
      "Epoch 106/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0713 - accuracy: 0.9844\n",
      "Epoch 107/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9844\n",
      "Epoch 108/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1279 - accuracy: 0.9688\n",
      "Epoch 109/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9766\n",
      "Epoch 110/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9844\n",
      "Epoch 111/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9922\n",
      "Epoch 112/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0556 - accuracy: 0.9844\n",
      "Epoch 113/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.9609\n",
      "Epoch 114/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1835 - accuracy: 0.9609\n",
      "Epoch 115/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9922\n",
      "Epoch 116/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0715 - accuracy: 0.9844\n",
      "Epoch 117/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.9766\n",
      "Epoch 118/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9844\n",
      "Epoch 119/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1042 - accuracy: 0.9766\n",
      "Epoch 120/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2645 - accuracy: 0.9453\n",
      "Epoch 121/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9922\n",
      "Epoch 122/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.9531\n",
      "Epoch 123/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.9609\n",
      "Epoch 124/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9922\n",
      "Epoch 125/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9922\n",
      "Epoch 126/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0617 - accuracy: 0.9844\n",
      "Epoch 127/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.9688\n",
      "Epoch 128/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0748 - accuracy: 0.9844\n",
      "Epoch 129/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1937 - accuracy: 0.9844\n",
      "Epoch 130/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9844\n",
      "Epoch 131/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.9844\n",
      "Epoch 132/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1328 - accuracy: 0.9766\n",
      "Epoch 133/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9922\n",
      "Epoch 134/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9844\n",
      "Epoch 135/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1423 - accuracy: 0.9688\n",
      "Epoch 136/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9922\n",
      "Epoch 137/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9844\n",
      "Epoch 139/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9922\n",
      "Epoch 140/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1793 - accuracy: 0.9766\n",
      "Epoch 141/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0709 - accuracy: 0.9922\n",
      "Epoch 142/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9922\n",
      "Epoch 143/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9844\n",
      "Epoch 144/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9766\n",
      "Epoch 145/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9766\n",
      "Epoch 146/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.9688\n",
      "Epoch 147/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9922\n",
      "Epoch 148/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9844\n",
      "Epoch 149/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1466 - accuracy: 0.9844\n",
      "Epoch 150/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9766\n",
      "Epoch 151/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1956 - accuracy: 0.9766\n",
      "Epoch 152/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9922\n",
      "Epoch 153/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 6.2209e-04 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9766\n",
      "Epoch 155/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0615 - accuracy: 0.9766\n",
      "Epoch 156/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.9844\n",
      "Epoch 157/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9922\n",
      "Epoch 158/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0923 - accuracy: 0.9766\n",
      "Epoch 159/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1599 - accuracy: 0.9766\n",
      "Epoch 160/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9844\n",
      "Epoch 161/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2120 - accuracy: 0.9844\n",
      "Epoch 162/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9922\n",
      "Epoch 163/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0965 - accuracy: 0.9688\n",
      "Epoch 164/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0595 - accuracy: 0.9844\n",
      "Epoch 165/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0589 - accuracy: 0.9844\n",
      "Epoch 166/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.2931 - accuracy: 0.9766\n",
      "Epoch 167/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1848 - accuracy: 0.9844\n",
      "Epoch 168/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9766\n",
      "Epoch 169/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9844\n",
      "Epoch 170/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9844\n",
      "Epoch 172/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9922\n",
      "Epoch 173/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9844\n",
      "Epoch 174/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1231 - accuracy: 0.9766\n",
      "Epoch 175/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0830 - accuracy: 0.9844\n",
      "Epoch 176/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.9922\n",
      "Epoch 177/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.9609\n",
      "Epoch 179/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 0.9844\n",
      "Epoch 181/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 1.7907e-04 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9844\n",
      "Epoch 183/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0547 - accuracy: 0.9922\n",
      "Epoch 184/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1327 - accuracy: 0.9844\n",
      "Epoch 185/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.9136 - accuracy: 0.9688\n",
      "Epoch 186/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9766\n",
      "Epoch 187/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9922\n",
      "Epoch 188/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9844\n",
      "Epoch 189/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.9922\n",
      "Epoch 190/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9922\n",
      "Epoch 191/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0493 - accuracy: 0.9844\n",
      "Epoch 192/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 4.2284e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 6.7027e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1087 - accuracy: 0.9844\n",
      "Epoch 195/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9922\n",
      "Epoch 196/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.9922\n",
      "Epoch 198/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1366 - accuracy: 0.9688\n",
      "Epoch 199/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9922\n",
      "Epoch 200/200\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(X[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(y[0]), activation='softmax'))\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "adam = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#fitting and saving the model \n",
    "hist = model.fit(X, y, epochs=200, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_pbA2JKt3RP0",
    "outputId": "200b5198-ad6e-4fd6-e92c-3611ee751fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chat_model.h5\t'GL Bot.json'\n"
     ]
    }
   ],
   "source": [
    "model.save('chat_model.h5')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LzkKgN713jc7"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('chat_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UegEZRhu3j63"
   },
   "source": [
    "Input data to bot also has to be processed in the same way how we train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DHLMYhqf3O4A"
   },
   "outputs": [],
   "source": [
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def pre_process_data(sentence, words):\n",
    "    sent_words = nltk.word_tokenize(sentence)\n",
    "    sent_words = [wordnet_lemmatizer.lemmatize(word.lower()) for word in sent_words]\n",
    "\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words) \n",
    "    for s in sent_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = pre_process_data(sentence, words)\n",
    "    train=p.reshape(158,1)\n",
    "    train=np.expand_dims(p,axis=0)\n",
    "    res = model.predict(train)[0]\n",
    "    thrshld = 0.2\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>thrshld]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"prob\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "8HIhRsOkNBIB"
   },
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):  \n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']  \n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(text):\n",
    "    idx = predict_class(text, model)\n",
    "    res = getResponse(idx, intents)\n",
    "    print(\"Bot:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "hqxhAsgPcZPV"
   },
   "outputs": [],
   "source": [
    "def chat_bot():\n",
    "    print(\"Bot: Hi there !! How can I help you today? (type quit to stop)!\")\n",
    "    while(True):\n",
    "      user_response = input(\"You: \")\n",
    "      user_response = user_response.lower()\n",
    "      if(user_response == 'quit'):\n",
    "         break\n",
    "      chatbot_response(user_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2710P64HiECv",
    "outputId": "1a26afe2-6a62-4c3e-d3ce-0cf751c72755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi there !! How can I help you today? (type quit to stop)!\n",
      "You: hiiii\n",
      "Bot: Hello! how can i help you ?\n",
      "You: olympus link\n",
      "Bot: Link: Olympus wiki\n",
      "You: what is softmax\n",
      "Bot: Link: Neural Nets wiki\n",
      "You: machine learning means\n",
      "Bot: Link: Machine Learning wiki \n",
      "You: thanks for link\n",
      "Bot: I hope I was able to assist you, Good Bye\n",
      "You: you are smart\n",
      "Bot: Please use respectful words\n",
      "You: you are stupid\n",
      "Bot: Please use respectful words\n",
      "You: very bad response\n",
      "Bot: Tarnsferring the request to your PM\n",
      "You: thanks bye\n",
      "Bot: I hope I was able to assist you, Good Bye\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat_bot()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prasad_NLP_project1_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
